{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open image files\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import ndimage\n",
    "\n",
    "d = 'sargentii_17'\n",
    "\n",
    "files = []\n",
    "for file in glob.glob(d+'/*.ti*'):\n",
    "    files.append(file)\n",
    "\n",
    "# open first image to get the image dimensions\n",
    "im = np.array(Image.open(files[0]))\n",
    "\n",
    "# define image_stack array\n",
    "image_stack = np.zeros([len(files),np.shape(im)[0],np.shape(im)[1]])\n",
    "# read and standardize all images\n",
    "for i in range(len(files)):\n",
    "    im = np.array(Image.open(files[i]))        \n",
    "\n",
    "    image_stack[i,:,:] = im\n",
    "    \n",
    "    \n",
    "# size of the tiles\n",
    "size = 50\n",
    "\n",
    "# number of simulations to run with the SVM\n",
    "n_sim = 3\n",
    "\n",
    "# variance retained for the PCA\n",
    "var = 0.99\n",
    "\n",
    "# the contour of every step_size-th frame will be searched\n",
    "step_size = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collect training data from every 100th frame\n",
    "# POLYGON\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#%matplotlib inline\n",
    "#matplotlib.rcParams.update({'font.size': 16})\n",
    "#matplotlib.rcParams['figure.figsize'] = (9.0, 6.0)\n",
    "from sklearn.svm import SVC\n",
    "from skimage.measure import moments\n",
    "import pickle\n",
    "\n",
    "frames = range(0,np.shape(image_stack)[2],50)\n",
    "\n",
    "# show the frames and click around the background/shell and shell/leaf boundary\n",
    "\n",
    "background_shell = []\n",
    "\n",
    "for f in frames:\n",
    "    # show image and click around the background/shell boundary first\n",
    "    # click as many times as you'd like. the first and last clicks will be connected to close the boundary\n",
    "    # press enter to terminate clicking\n",
    "    plt.title('Click on the background - shell boundary')\n",
    "    plt.axis('equal')\n",
    "    plt.xlim([0,np.shape(image_stack[:,:,f])[0]])\n",
    "    plt.ylim([0,np.shape(image_stack[:,:,f])[1]])\n",
    "    plt.imshow(image_stack[:,:,f].T,cmap='Greys_r')\n",
    "    coord = plt.ginput(n=-1,timeout=-1)\n",
    "    plt.close()\n",
    "    background_shell.append(coord)\n",
    "\n",
    "\n",
    "shell_leaf = []\n",
    "for f in frames:\n",
    "    # show image and click around the shell-leaf boundary \n",
    "    # click as many times as you'd like. the first and last clicks will be connected to close the boundary\n",
    "    # press enter to terminate clicking\n",
    "    plt.title('Click on the shell-leaf boundary')\n",
    "    plt.axis('equal')\n",
    "    plt.xlim([0,np.shape(image_stack[:,:,f])[0]])\n",
    "    plt.ylim([0,np.shape(image_stack[:,:,f])[1]])\n",
    "    plt.imshow(image_stack[:,:,f].T,cmap='Greys_r')\n",
    "    coord = plt.ginput(n=-1,timeout=-1)\n",
    "    plt.close()\n",
    "    shell_leaf.append(coord)\n",
    "\n",
    "file_name = 'data_files/'+d+'_shell_leaf_boundaries_d3.dat'\n",
    "f = open(file_name,'w')\n",
    "pickle.dump([background_shell,shell_leaf],f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n",
      "1 0 361 180\n",
      "2 0 844 422\n",
      "3 238 932 466\n",
      "4 544 866 433\n",
      "5 945 642 321\n",
      "6 1008 608 304\n",
      "7 861 602 301\n",
      "8 905 469 234\n",
      "9 594 483 241\n",
      "10 0 600 300\n",
      "11 0 358 179\n",
      "12 0 37 18\n",
      "(50, 50, 15296)\n",
      "(15296,)\n",
      "5095 6802 3399\n"
     ]
    }
   ],
   "source": [
    "# get the training data\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rcParams['figure.figsize'] = (9.0, 6.0)\n",
    "from sklearn.svm import SVC\n",
    "from skimage.measure import moments\n",
    "import pickle\n",
    "from skimage.measure import points_in_poly\n",
    "\n",
    "frames = range(0,np.shape(image_stack)[2],50)\n",
    "\n",
    "file_name = 'data_files/'+d+'_shell_leaf_boundaries_d3.dat'\n",
    "f = open(file_name,'r')\n",
    "[background_shell,shell_leaf] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "tiles = np.empty(shape=[size,size,0])\n",
    "labels = np.empty(shape=[0])\n",
    "\n",
    "for f in range(len(frames)):\n",
    "\n",
    "    # calculate leaf and shell areas\n",
    "    coord = shell_leaf[f]\n",
    "    if len(coord) > 0:\n",
    "        x = [c[0] for c in coord]\n",
    "        y = [c[1] for c in coord]\n",
    "        x_leaf = np.append(x,x[0])\n",
    "        y_leaf = np.append(y,y[0])\n",
    "        area_leaf = np.abs(np.sum(x_leaf[:-1]*y_leaf[1:]-y_leaf[:-1]*x_leaf[1:]))/2e0\n",
    "        \n",
    "        # expand the boundary by tile size / 3\n",
    "        center_x = np.mean(x_leaf)\n",
    "        center_y = np.mean(y_leaf)\n",
    "        length = np.sqrt((x_leaf-center_x)**2e0 + (y_leaf-center_y)**2e0)\n",
    "        x_leaf_plus = x_leaf + (x_leaf - center_x)/length*size/2e0\n",
    "        y_leaf_plus = y_leaf + (y_leaf - center_y)/length*size/2e0\n",
    "        # shrink the boundary by tile size / 3\n",
    "        x_leaf_minus = x_leaf - (x_leaf - center_x)/length*size/2e0\n",
    "        y_leaf_minus = y_leaf - (y_leaf - center_y)/length*size/2e0\n",
    "        \n",
    "    else:\n",
    "        x_leaf = 0\n",
    "        y_leaf = 0\n",
    "        x_leaf_plus = 0\n",
    "        y_leaf_plus = 0\n",
    "        x_leaf_minus = 0\n",
    "        y_leaf_minus = 0\n",
    "        area_leaf = 0e0\n",
    "\n",
    "    coord = background_shell[f]\n",
    "    if len(coord) > 0:\n",
    "        x = [c[0] for c in coord]\n",
    "        y = [c[1] for c in coord]\n",
    "        x_shell = np.append(x,x[0])\n",
    "        y_shell = np.append(y,y[0])\n",
    "        # expand the boundary by the size of the tile\n",
    "        center_x = np.mean(x_shell)\n",
    "        center_y = np.mean(y_shell)\n",
    "        length = np.sqrt((x_shell-center_x)**2e0 + (y_shell-center_y)**2e0)\n",
    "        x_shell_plus = x_shell + (x_shell - center_x)/length*size*2e0/3e0\n",
    "        y_shell_plus = y_shell + (y_shell - center_y)/length*size*2e0/3e0\n",
    "\n",
    "        area_shell = np.abs(np.sum(x_shell[:-1]*y_shell[1:]-y_shell[:-1]*x_shell[1:]))/2e0 - area_leaf\n",
    "    \n",
    "    else:\n",
    "        x_shell = 0\n",
    "        y_shell = 0\n",
    "        x_shell_plus = 0\n",
    "        y_shell_plus = 0\n",
    "        area_shell = 0e0\n",
    "    \n",
    "    # estimate number of tiles based on the area\n",
    "\n",
    "    n_leaf = int(area_leaf/200)\n",
    "    n_shell = int(area_shell/400)\n",
    "    n_background = int(n_shell/2)\n",
    "    \n",
    "    print f,n_leaf,n_shell,n_background\n",
    "\n",
    "    leaf = np.zeros([size,size,n_leaf])\n",
    "    shell = np.zeros([size,size,n_shell])\n",
    "    background = np.zeros([size,size,n_background])\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    k = 0\n",
    "\n",
    "    while i < n_leaf or j < n_shell or k < n_background:\n",
    "\n",
    "        # get random pixel from the image\n",
    "        x = np.random.randint(np.shape(image_stack[:,:,frames[f]])[0]-size)\n",
    "        y = np.random.randint(np.shape(image_stack[:,:,frames[f]])[1]-size)\n",
    "\n",
    "        # check whether all the corners of the square fall within any of the regions\n",
    "        \n",
    "        corners = np.zeros([4,2])\n",
    "        corners[0,:] = [x,y]\n",
    "        corners[1,:] = [x+size,y]\n",
    "        corners[2,:] = [x,y+size]\n",
    "        corners[3,:] = [x+size,y+size]    \n",
    "        \n",
    "        \n",
    "        if np.all(points_in_poly(corners,np.column_stack((x_leaf_plus,y_leaf_plus)))) and i < n_leaf:\n",
    "            # leaf\n",
    "            # get the hog features of this cell\n",
    "            leaf[:,:,i] = image_stack[x:x+size,y:y+size,frames[f]]\n",
    "            square = plt.Rectangle((x,y),size,size,color='r',fill=False)\n",
    "            plt.gca().add_patch(square)\n",
    "            i = i + 1\n",
    "\n",
    "\n",
    "        if np.all(points_in_poly(corners,np.column_stack((x_shell_plus,y_shell_plus)))) and np.all(~points_in_poly(corners,np.column_stack((x_leaf_minus,y_leaf_minus)))) and j < n_shell:\n",
    "            # shell\n",
    "            shell[:,:,j] = image_stack[x:x+size,y:y+size,frames[f]]\n",
    "            square = plt.Rectangle((x,y),size,size,color='y',fill=False)\n",
    "            plt.gca().add_patch(square)\n",
    "            j = j + 1\n",
    "\n",
    "\n",
    "        if np.all(~points_in_poly(corners,np.column_stack((x_shell_plus,y_shell_plus)))) and k < n_background:\n",
    "            # background\n",
    "            background[:,:,k] = image_stack[x:x+size,y:y+size,frames[f]]\n",
    "            square = plt.Rectangle((x,y),size,size,color='b',fill=False)\n",
    "            plt.gca().add_patch(square)\n",
    "            k = k + 1\n",
    "\n",
    "    tiles = np.append(tiles,np.concatenate((leaf,shell,background),axis=2),axis=2)\n",
    "    label = np.concatenate((np.zeros(n_leaf)+1,np.zeros(n_shell)+2,np.zeros(n_background)))\n",
    "    labels = np.append(labels,label)\n",
    "\n",
    "    plt.axis('equal')\n",
    "\n",
    "    plt.xlim([0,np.shape(image_stack)[0]])\n",
    "    plt.ylim([0,np.shape(image_stack)[1]])\n",
    "    plt.imshow(image_stack[:,:,frames[f]].T,cmap='Greys_r')\n",
    "    plt.savefig('imgs/shell_leaf_boundaries_'+str(frames[f])+'_size'+str(size)+'_d3.png',dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "print np.shape(tiles)\n",
    "print np.shape(labels)\n",
    "print len(labels[labels == 1]),len(labels[labels == 2]),len(labels[labels == 0])\n",
    "\n",
    "# rotate the tiles and pickle dump the arrays\n",
    "#new_tiles = np.append(tiles,np.rot90(tiles,k=1),axis=2)\n",
    "#new_labels = np.append(labels,labels)\n",
    "\n",
    "#new_tiles = np.append(new_tiles,np.rot90(tiles,k=2),axis=2)\n",
    "#new_labels = np.append(new_labels,labels)\n",
    "\n",
    "#new_tiles = np.append(new_tiles,np.rot90(tiles,k=3),axis=2)\n",
    "#new_labels = np.append(new_labels,labels)\n",
    "\n",
    "#print np.shape(new_tiles)\n",
    "#print np.shape(new_labels)\n",
    "\n",
    "file_name = 'data_files/training_data_'+str(size)+'x'+str(size)+'_d3.dat'\n",
    "f = open(file_name,'w')\n",
    "pickle.dump([tiles,labels],f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:30: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.40 % variance retained in 5 dimensions\n",
      "85.21 % variance retained in 9 dimensions\n",
      "90.12 % variance retained in 19 dimensions\n",
      "95.02 % variance retained in 57 dimensions\n",
      "99.00 % variance retained in 295 dimensions\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "#do PCA\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "'''\n",
    "Performs the Principal Coponent analysis of the Matrix X\n",
    "Matrix must be n * m dimensions\n",
    "where n is # features\n",
    "m is # examples\n",
    "'''\n",
    "\n",
    "def PCA(X, varRetained = [0.95],filename = 'PCA_data.dat'):\n",
    "\n",
    "    # Compute Covariance Matrix Sigma\n",
    "    (n, m) = X.shape\n",
    "\n",
    "    Sigma = 1.0 / float(m) * np.dot(X, np.transpose(X))\n",
    "    # Compute eigenvectors and eigenvalues of Sigma\n",
    "    U, s, V = np.linalg.svd(Sigma)\n",
    "\n",
    "    # compute the value k: number of minumum features that \n",
    "    # retains the given variance\n",
    "    s_tot = np.sum(s)\n",
    "        \n",
    "    var_i = np.array([np.sum(s[: i + 1]) / s_tot * 100.0 for i in range(n)])\n",
    "    \n",
    "    k = np.zeros(len(varRetained))\n",
    "    for i in range(len(k)):\n",
    "        k[i] = len(var_i[var_i < (varRetained[i] * 100e0)])\n",
    "\n",
    "        print '%.2f %% variance retained in %d dimensions' % (var_i[k[i]], k[i])\n",
    "\n",
    "        # compute the reduced dimensional features \n",
    "        U_reduced = U[:, : k[i]]\n",
    "        Z = np.dot(np.transpose(U_reduced),X)\n",
    "\n",
    "        # pickle dump the results\n",
    "        f = open(filename+str(int(varRetained[i]*100e0))+'.dat','w')\n",
    "        pickle.dump([Z, U_reduced, k[i]],f)\n",
    "        f.close() \n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "var_ret = [0.80, 0.85, 0.90, 0.95, 0.99] \n",
    "\n",
    "# load the training data and divide it to training (60%), test (20%), and cross validation (20%) sets\n",
    "f = open('data_files/training_data_'+str(size)+'x'+str(size)+'_d3.dat','r')\n",
    "[tiles,labels] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "# standardize tiles\n",
    "tiles_standard = np.zeros(np.shape(tiles))\n",
    "\n",
    "mean = np.mean(image_stack)\n",
    "std = np.std(image_stack)\n",
    "for i in range(len(labels)):\n",
    "    tiles_standard[:,:,i] = (tiles[:,:,i] - mean) / std\n",
    "\n",
    "# reshape tiles\n",
    "reshape_tiles = np.reshape(tiles_standard,[size*size,len(labels)])\n",
    "\n",
    "filename = 'data_files/PCA_data_d3_'+str(size)+'x'+str(size)+'_var0'\n",
    "\n",
    "# do PCA and save the results\n",
    "PCA(reshape_tiles,varRetained = var_ret,filename = filename)\n",
    "\n",
    "print 'finished'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0\n",
      "   best C value(s): [ 10.]\n",
      "   best gamma value(s): [ 0.001]\n",
      "   max test score: 0.864007845701\n",
      "    1\n",
      "   best C value(s): [ 10.]\n",
      "   best gamma value(s): [ 0.001]\n",
      "   max test score: 0.852893102321\n",
      "    2\n",
      "   best C value(s): [ 10.]\n",
      "   best gamma value(s): [ 0.001]\n",
      "   max test score: 0.864988558352\n",
      "    3\n",
      "   best C value(s): [ 10.]\n",
      "   best gamma value(s): [ 0.001]\n",
      "   max test score: 0.84635501798\n",
      "    4\n",
      "   best C value(s): [ 10.]\n",
      "   best gamma value(s): [ 0.001]\n",
      "   max test score: 0.850277868585\n",
      "    5\n",
      "   best C value(s): [ 10.]\n",
      "   best gamma value(s): [ 0.001]\n",
      "   max test score: 0.858450474011\n",
      "    6\n",
      "   best C value(s): [ 10.]\n",
      "   best gamma value(s): [ 0.001]\n",
      "   max test score: 0.865969271004\n",
      "    7\n",
      "   best C value(s): [ 10.]\n",
      "   best gamma value(s): [ 0.001]\n",
      "   max test score: 0.852893102321\n",
      "    8\n",
      "   best C value(s): [ 10.]\n",
      "   best gamma value(s): [ 0.001]\n",
      "   max test score: 0.860738803531\n",
      "    9\n",
      "   best C value(s): [ 10.]\n",
      "   best gamma value(s): [ 0.001]\n",
      "   max test score: 0.861392611965\n",
      "best C value(s): [ 10.]\n",
      "best gamma value(s): [ 0.001]\n",
      "max test score: 0.857796665577\n",
      "CV score: [ 0.85428105] +/- [ 0.0044412]\n"
     ]
    }
   ],
   "source": [
    "# run the SVM\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rcParams['figure.figsize'] = (9.0, 6.0)\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "from scipy.misc import imresize\n",
    "\n",
    "file_name = 'data_files/training_data_'+str(size)+'x'+str(size)+'_d3.dat'\n",
    "f = open(file_name,'r')\n",
    "[X,Y] = pickle.load(f)\n",
    "f.close()  \n",
    "\n",
    "\n",
    "f = open('data_files/PCA_data_d3_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'.dat','r')\n",
    "[Z, U_reduced, k] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "# range of C and gamma values\n",
    "# the range used to be wider: -8e0,3e0 with num = 12\n",
    "# experience showed that the best C is within this reduced range usually\n",
    "C = 10e0**(np.linspace(0e0,4e0,5))\n",
    "gamma = 10e0**(np.linspace(-4e0,0e0,5))\n",
    "\n",
    "train_score = np.zeros([n_sim,len(C),len(gamma)])\n",
    "test_score = np.zeros([n_sim,len(C),len(gamma)])\n",
    "CV_score = np.zeros([n_sim,len(C),len(gamma)])\n",
    "\n",
    "for i in range(n_sim):\n",
    "    print '   ',i\n",
    "    # shuffle and split data\n",
    "    indx = np.arange(np.shape(Z.T)[0])\n",
    "    np.random.shuffle(indx)\n",
    "    # split 60-20-20%\n",
    "    split1 = int(np.shape(Z.T)[0]*0.6)\n",
    "    split2 = int(np.shape(Z.T)[0]*0.8)\n",
    "    X_train = Z.T[indx[:split1]]\n",
    "    Y_train = Y[indx[:split1]]\n",
    "    X_test = Z.T[indx[split1:split2]]\n",
    "    Y_test = Y[indx[split1:split2]]\n",
    "    X_CV = Z.T[indx[split2:]]\n",
    "    Y_CV = Y[indx[split2:]]\n",
    "\n",
    "\n",
    "    # train SVM and loop through all C and gamma combinations\n",
    "    for j in range(len(C)):\n",
    "        for k in range(len(gamma)):\n",
    "            #print '      C and gamma:',C[j],gamma[k]\n",
    "            SVC_bud = SVC(kernel='rbf', C=C[j], gamma=gamma[k]).fit(X_train, Y_train)            \n",
    "            train_score[i,j,k] = SVC_bud.score(X_train,Y_train)\n",
    "            test_score[i,j,k] = SVC_bud.score(X_test,Y_test)\n",
    "            CV_score[i,j,k] = SVC_bud.score(X_CV,Y_CV)\n",
    "\n",
    "\n",
    "    # find the C and gamma parameters that give max score.\n",
    "    # if there are multiple parameter configuration giving max score, the first one of these is used below \n",
    "    best_params = np.where(test_score[i,:,:] == np.max(test_score[i,:,:]))\n",
    "    print '   best C value(s):',C[best_params[0]]\n",
    "    print '   best gamma value(s):',gamma[best_params[1]]\n",
    "    print '   max test score:',np.max(test_score[i,:,:])\n",
    "\n",
    "\n",
    "\n",
    "avg_score = np.mean(test_score[:,:,:],axis=0)\n",
    "avg_CV_score = np.mean(CV_score[:,:,:],axis=0)\n",
    "std_CV_score = np.std(CV_score[:,:,:],axis=0)\n",
    "\n",
    "best_params = np.where(avg_score == np.max(avg_score))\n",
    "print 'best C value(s):',C[best_params[0]]\n",
    "print 'best gamma value(s):',gamma[best_params[1]]\n",
    "print 'max test score:',np.max(avg_score)\n",
    "\n",
    "print 'CV score:',avg_CV_score[best_params],'+/-',std_CV_score[best_params]\n",
    "# train the best SVM and save it\n",
    "SVC_bud = SVC(kernel='rbf', C=C[best_params[0][0]], gamma=gamma[best_params[1][0]]).fit(Z.T, Y)            \n",
    "\n",
    "\n",
    "# save the results\n",
    "f = open('data_files/SVM_tile_scores_nsim'+str(n_sim)+'_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'_d3.dat','w')\n",
    "pickle.dump([test_score,train_score,CV_score,SVC_bud,C,gamma],f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# go through the image stack and render the classes\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rcParams['figure.figsize'] = (9.0, 6.0)\n",
    "from sklearn.svm import SVC\n",
    "from skimage.util.shape import view_as_windows\n",
    "import pickle\n",
    "from skimage import measure\n",
    "from skimage.filters import gaussian_filter\n",
    "\n",
    "f = open('data_files/PCA_data_d3_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'.dat','r')\n",
    "[Z, U_reduced, k] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('data_files/SVM_tile_scores_nsim10_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'_d3.dat','r')\n",
    "[test_score,train_score,CV_score,SVC_bud,C,gamma] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "windows = view_as_windows(image_stack[:,:,0],(size,size))\n",
    "\n",
    "f_range = np.arange(np.shape(image_stack)[2],step=step_size)\n",
    "i_range = np.arange(np.shape(windows)[0],step=step_size)\n",
    "j_range = np.arange(np.shape(windows)[1],step=step_size)\n",
    "\n",
    "class_stack = np.zeros([len(f_range),len(i_range),len(j_range)]).astype(int)\n",
    "\n",
    "mean_im = np.mean(image_stack)\n",
    "std_im = np.std(image_stack)\n",
    "for f in range(len(f_range)):\n",
    "    if f%(100/step_size) == 0:\n",
    "        print f\n",
    "\n",
    "    windows = view_as_windows(image_stack[:,:,f_range[f]],(size,size))\n",
    "    \n",
    "    PCA_features = np.zeros([len(i_range),len(j_range),np.shape(Z)[0]])\n",
    "    # collect the PCA features\n",
    "    for i in range(len(i_range)):\n",
    "        for j in range(len(j_range)):\n",
    "            tile_standard = (windows[i_range[i],j_range[j]] - mean_im)/std_im\n",
    "            reshape_tile = tile_standard.reshape(size*size)\n",
    "            PCA_features[i,j,:] = np.dot(reshape_tile,U_reduced)\n",
    "\n",
    "    # predict the classes:\n",
    "    for i in range(len(i_range)):\n",
    "        class_stack[f,i,:] = SVC_bud.predict(PCA_features[i,:,:])\n",
    "        \n",
    "    class_stack[f,:,:][class_stack[f,:,:] == 2] = 0\n",
    "    \n",
    "    if f%(100/step_size) == 0:\n",
    "    \n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.axis('equal')\n",
    "        #plt.xlim([0,np.shape(image_stack[:,:,f_range[f]])[0]])\n",
    "        #plt.ylim([0,np.shape(image_stack[:,:,f_range[f]])[1]])\n",
    "        plt.imshow(image_stack[:,:,f_range[f]],cmap='Greys_r')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.axis('equal')\n",
    "        #plt.xlim([0,np.shape(class_stack[f,:,:])[0]])\n",
    "        #plt.ylim([0,np.shape(class_stack[f,:,:])[1]])\n",
    "        plt.imshow(class_stack[f,:,:])\n",
    "        plt.savefig('animation_border/contours_size'+str(size)+'_tile'+str(f_range[f])+'_d3.png',dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "f = open('data_files/class_stack_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'_d3.dat','w')\n",
    "pickle.dump([class_stack],f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:49: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# go through the class stack and select the contours\n",
    "# ffmpeg -framerate 25 -i contours_tiles_frame%04d_size50.png -s:v 750x750 -c:v libx264 -profile:v high -crf 20 -pix_fmt yuv420p ani_contours.mp4\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "matplotlib.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "from sklearn.svm import SVC\n",
    "from skimage.util.shape import view_as_windows\n",
    "import pickle\n",
    "from skimage import measure\n",
    "from skimage.filters import gaussian_filter\n",
    "\n",
    "f = open('data_files/class_stack_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'_d3.dat','r')\n",
    "[class_stack] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "for f in range(np.shape(image_stack)[2]):\n",
    "    \n",
    "    if f%50 == 0:\n",
    "        print f\n",
    "    \n",
    "    smoothed = gaussian_filter(class_stack[f/step_size,:,:].astype(float),sigma=2)\n",
    "    \n",
    "    contours = measure.find_contours(smoothed, 0.5)\n",
    "    \n",
    "    plt.axis('equal')\n",
    "    plt.xlim([0,np.shape(image_stack[:,:,f])[0]])\n",
    "    plt.ylim([0,np.shape(image_stack[:,:,f])[1]])\n",
    "    \n",
    "    if len(contours) != 0:\n",
    "    \n",
    "        # calculate the area of the contours\n",
    "        area = np.zeros(len(contours))\n",
    "\n",
    "        for n, contour in enumerate(contours):\n",
    "            x = contour[:,0]\n",
    "            y = contour[:,1]\n",
    "            # copy the first point to the end of the contour list to close the loop\n",
    "            x = np.append(x,x[0])\n",
    "            y = np.append(y,y[0])\n",
    "            # use Green's theorem to calculate the area of the contour\n",
    "            area[n] = np.abs(np.sum(x[:-1]*y[1:]-y[:-1]*x[1:]))/2e0\n",
    "\n",
    "        # find contour with the largest area\n",
    "        cont = contours[np.where(area == np.max(area))[0]]\n",
    "\n",
    "        plt.plot(size/2e0+cont[:, 1]*step_size, size/2e0+cont[:, 0]*step_size,color='r', linewidth=2) \n",
    "        #for n, contour in enumerate(contours):\n",
    "        #    plt.plot(size/2e0+contour[:, 1]*step_size, size/2e0+contour[:, 0]*step_size,color='r', linewidth=1) \n",
    "\n",
    "    plt.imshow(image_stack[:,:,f],cmap='Greys_r',vmin=0,vmax=66000)\n",
    "    plt.savefig('animation_border/tile'+str(size)+'/contours_tiles_frame'+str(f).zfill(4)+'_size'+str(size)+'_d3.jpg',dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:52: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "[3L, 310L, 334L, 496L, 1L, 1L, 1L, 1L]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7920cebcfa21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mnim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_files/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_boundary_half_res_size'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.nii.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# pixels outside the boundary are set to 0 and a nifti file is produced\n",
    "# the nifti file can be loaded with ITK SNAP to see the 3D structure of the leaf\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "matplotlib.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "from sklearn.svm import SVC\n",
    "from skimage.util.shape import view_as_windows\n",
    "import pickle\n",
    "from skimage.measure import grid_points_in_poly\n",
    "from skimage.measure import find_contours\n",
    "from skimage.filters import gaussian_filter\n",
    "from nifti import *\n",
    "import nifti.clib as ncl\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "\n",
    "f = open('data_files/class_stack_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'_d3.dat','r')\n",
    "[class_stack] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "boundary = np.zeros(np.shape(image_stack))\n",
    "\n",
    "for f in range(np.shape(image_stack)[2]):\n",
    "    \n",
    "    if f%50 == 0:\n",
    "        print f\n",
    "    \n",
    "    smoothed = gaussian_filter(class_stack[f/step_size,:,:].astype(float),sigma=2)\n",
    "    \n",
    "    contours = find_contours(smoothed, 0.5)\n",
    "    \n",
    "    if len(contours) != 0:\n",
    "    \n",
    "        # calculate the area of the contours\n",
    "        area = np.zeros(len(contours))\n",
    "\n",
    "        for n, contour in enumerate(contours):\n",
    "            x = contour[:,0]\n",
    "            y = contour[:,1]\n",
    "            # copy the first point to the end of the contour list to close the loop\n",
    "            x = np.append(x,x[0])\n",
    "            y = np.append(y,y[0])\n",
    "            # use Green's theorem to calculate the area of the contour\n",
    "            area[n] = np.abs(np.sum(x[:-1]*y[1:]-y[:-1]*x[1:]))/2e0\n",
    "\n",
    "        # find contour with the largest area\n",
    "        cont = contours[np.where(area == np.max(area))[0]]\n",
    "    \n",
    "    pixels_in_cont = grid_points_in_poly(np.shape(image_stack[:,:,f]),cont*step_size+size/2e0)\n",
    "    \n",
    "    boundary[:,:,f] = image_stack[:,:,f]*pixels_in_cont\n",
    "    \n",
    "    if f%100 == 0:\n",
    "    \n",
    "        plt.axis('equal')\n",
    "        plt.xlim([0,np.shape(image_stack[:,:,f])[0]])\n",
    "        plt.ylim([0,np.shape(image_stack[:,:,f])[1]])\n",
    "        plt.imshow(boundary[f,:,:],cmap='Greys_r',vmin=0,vmax=66000)\n",
    "        plt.savefig('imgs/boundary_tiles_frame'+str(f).zfill(4)+'_size'+str(size)+'_d3.jpg',dpi=100)\n",
    "        plt.close()\n",
    "    \n",
    "\n",
    "nim = NiftiImage(zoom(boundary,0.5))\n",
    "print nim.header['dim']\n",
    "nim.header['datatype'] == ncl.NIFTI_TYPE_FLOAT64\n",
    "nim.save('data_files/'+d+'_boundary_half_res_size'+str(size)+'_d3.nii.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the segmented nifti image and make an animation\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "from sklearn.svm import SVC\n",
    "from skimage.util.shape import view_as_windows\n",
    "import pickle\n",
    "from skimage.measure import grid_points_in_poly\n",
    "from skimage.measure import find_contours\n",
    "from skimage.filters import gaussian_filter\n",
    "from nifti import *\n",
    "import nifti.clib as ncl\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "\n",
    "nim = NiftiImage('data_files/segmented_sargentii_17_size60_d3.nii.gz').asarray()\n",
    "\n",
    "for i in range(np.shape(image_stack)[2]):\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.axis('equal')\n",
    "    plt.title('original image')\n",
    "    plt.imshow(image_stack[:,:,i],cmap='Greys_r',vmin=0,vmax=66000)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.axis('equal')\n",
    "    plt.title('segmented image - leaf')\n",
    "    plt.imshow(zoom(nim[:,:,i/2],2e0),cmap='Greys_r')\n",
    "    plt.savefig('animation_border/segmented/segmented_size'+str(size)+'_frame'+str(i).zfill(4)+'_d3.png',dpi=100)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run marching cubes and rotate the leaf around in the animation\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mayavi import mlab\n",
    "from nifti import *\n",
    "import nifti.clib as ncl\n",
    "\n",
    "nim = NiftiImage('data_files/segmented_sargentii_17_size60_d3.nii.gz').asarray()\n",
    "\n",
    "# march the cubes\n",
    "verts, faces = measure.marching_cubes(np.swapaxes(nim,0,2), 0.5)\n",
    "\n",
    "for i in range(360):\n",
    "    # make the plot with mlab\n",
    "    mlab.triangular_mesh([vert[0] for vert in verts],\n",
    "              [vert[1] for vert in verts],\n",
    "              [vert[2] for vert in verts], faces) \n",
    "    mlab.view(azimuth = i)\n",
    "    mlab.savefig('animation_border/segmented/marching_cubes_size'+str(size)+'_'+str(i).zfill(3)+'_d3.png')\n",
    "    mlab.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
