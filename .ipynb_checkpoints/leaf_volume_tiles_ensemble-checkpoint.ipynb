{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open image files and set parameters\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from skimage.measure import moments\n",
    "import pickle\n",
    "from skimage.measure import points_in_poly\n",
    "from scipy.misc import imresize\n",
    "from skimage.util.shape import view_as_windows\n",
    "from skimage import measure\n",
    "from skimage.filters import gaussian_filter\n",
    "from skimage.measure import grid_points_in_poly\n",
    "from skimage.measure import find_contours\n",
    "from skimage.filters import gaussian_filter\n",
    "from nifti import *\n",
    "import nifti.clib as ncl\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import powerlaw as sp_power\n",
    "\n",
    "d = 'sample17_batch3_IJ_stack_ok'\n",
    "\n",
    "files = []\n",
    "for file in glob.glob('image_stacks/'+d+'/*.ti*'):\n",
    "    files.append(file)\n",
    "\n",
    "files.sort()\n",
    "\n",
    "# open first image to get the image dimensions\n",
    "im = np.array(Image.open(files[0]))\n",
    "\n",
    "# define image_stack array\n",
    "image_stack = np.zeros([len(files),np.shape(im)[0],np.shape(im)[1]])\n",
    "# read and standardize all images\n",
    "for i in range(len(files)):\n",
    "    im = np.array(Image.open(files[i]))\n",
    "    \n",
    "    image_stack[i,:,:] = im\n",
    "\n",
    "# size of the tiles\n",
    "size = 50\n",
    "\n",
    "# variance retained for the PCA\n",
    "#var = 0.99\n",
    "#var = 0.95\n",
    "var = 0.90\n",
    "\n",
    "# the contour of every step_size-th frame will be searched\n",
    "step_size = 10\n",
    "\n",
    "# number of frames to click through\n",
    "nr_frames = 20\n",
    "\n",
    "# paramteres for RandomizedSearchCV\n",
    "n_iter = 100\n",
    "k_fold = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 478 801 801\n",
      "1 576 719 719\n",
      "2 688 755 755\n",
      "3 812 676 676\n",
      "4 919 711 711\n",
      "5 1014 729 729\n",
      "6 1021 701 701\n",
      "7 1023 679 679\n",
      "8 969 703 703\n",
      "9 788 735 735\n",
      "10 702 703 703\n",
      "11 615 680 680\n",
      "12 507 644 644\n",
      "13 332 658 658\n",
      "14 0 727 727\n",
      "15 0 588 588\n",
      "16 0 472 472\n",
      "17 0 357 357\n",
      "18 0 204 204\n",
      "19 0 79 79\n",
      "(50, 50, 35086)\n",
      "(35086,)\n",
      "10444 12321 12321\n"
     ]
    }
   ],
   "source": [
    "# get the training data\n",
    "\n",
    "file_name = 'data_files/'+d+'_shell_leaf_boundaries_d1.dat'\n",
    "f = open(file_name,'r')\n",
    "[background_shell,shell_leaf,fs] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "frames = np.linspace(0,len(files),num=nr_frames,endpoint=False).astype(int)\n",
    "\n",
    "tiles = np.empty(shape=[size,size,0])\n",
    "labels = np.empty(shape=[0])\n",
    "\n",
    "for f in range(len(frames)):\n",
    "    \n",
    "    # calculate leaf and shell areas\n",
    "    coord = shell_leaf[f]\n",
    "    if len(coord) > 0:\n",
    "        x = [c[0] for c in coord]\n",
    "        y = [c[1] for c in coord]\n",
    "        x_leaf = np.append(x,x[0])\n",
    "        y_leaf = np.append(y,y[0])\n",
    "        area_leaf = np.abs(np.sum(x_leaf[:-1]*y_leaf[1:]-y_leaf[:-1]*x_leaf[1:]))/2e0\n",
    "        \n",
    "        # expand the boundary by tile size / 3\n",
    "        center_x = np.mean(x_leaf)\n",
    "        center_y = np.mean(y_leaf)\n",
    "        length = np.sqrt((x_leaf-center_x)**2e0 + (y_leaf-center_y)**2e0)\n",
    "        x_leaf_plus = x_leaf + (x_leaf - center_x)/length*size/2e0\n",
    "        y_leaf_plus = y_leaf + (y_leaf - center_y)/length*size/2e0\n",
    "        # shrink the boundary by tile size / 3\n",
    "        x_leaf_minus = x_leaf - (x_leaf - center_x)/length*size/2e0\n",
    "        y_leaf_minus = y_leaf - (y_leaf - center_y)/length*size/2e0\n",
    "    \n",
    "    else:\n",
    "        x_leaf = 0\n",
    "        y_leaf = 0\n",
    "        x_leaf_plus = 0\n",
    "        y_leaf_plus = 0\n",
    "        x_leaf_minus = 0\n",
    "        y_leaf_minus = 0\n",
    "        area_leaf = 0e0\n",
    "    \n",
    "    coord = background_shell[f]\n",
    "    if len(coord) > 0:\n",
    "        x = [c[0] for c in coord]\n",
    "        y = [c[1] for c in coord]\n",
    "        x_shell = np.append(x,x[0])\n",
    "        y_shell = np.append(y,y[0])\n",
    "        # expand the boundary by the size of the tile\n",
    "        center_x = np.mean(x_shell)\n",
    "        center_y = np.mean(y_shell)\n",
    "        length = np.sqrt((x_shell-center_x)**2e0 + (y_shell-center_y)**2e0)\n",
    "        x_shell_plus = x_shell + (x_shell - center_x)/length*size*2e0/3e0\n",
    "        y_shell_plus = y_shell + (y_shell - center_y)/length*size*2e0/3e0\n",
    "        \n",
    "        area_shell = np.abs(np.sum(x_shell[:-1]*y_shell[1:]-y_shell[:-1]*x_shell[1:]))/2e0 - area_leaf\n",
    "    \n",
    "    else:\n",
    "        x_shell = 0\n",
    "        y_shell = 0\n",
    "        x_shell_plus = 0\n",
    "        y_shell_plus = 0\n",
    "        area_shell = 0e0\n",
    "    \n",
    "    # estimate number of tiles based on the area\n",
    "    \n",
    "    if var == 0.99:\n",
    "        n_leaf = int(area_leaf/200)\n",
    "        n_shell = int(area_shell/400)\n",
    "        n_background = int(n_shell/2)\n",
    "    if var == 0.95:\n",
    "        n_leaf = int(area_leaf/100)\n",
    "        n_shell = int(area_shell/200)\n",
    "        n_background = int(n_shell/1)\n",
    "    if var == 0.90\n",
    "        n_leaf = int(area_leaf/40)\n",
    "        n_shell = int(area_shell/80)\n",
    "        n_background = int(n_shell*2.5)\n",
    "    \n",
    "    print f,n_leaf,n_shell,n_background\n",
    "    \n",
    "    leaf = np.zeros([size,size,n_leaf])\n",
    "    shell = np.zeros([size,size,n_shell])\n",
    "    background = np.zeros([size,size,n_background])\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    k = 0\n",
    "    \n",
    "    while i < n_leaf or j < n_shell or k < n_background:\n",
    "        \n",
    "        # get random pixel from the image\n",
    "        x = np.random.randint(np.shape(image_stack[frames[f],:,:])[0]-size)\n",
    "        y = np.random.randint(np.shape(image_stack[frames[f],:,:])[1]-size)\n",
    "        \n",
    "        # check whether all the corners of the square fall within any of the regions\n",
    "        \n",
    "        corners = np.zeros([4,2])\n",
    "        corners[0,:] = [x,y]\n",
    "        corners[1,:] = [x+size,y]\n",
    "        corners[2,:] = [x,y+size]\n",
    "        corners[3,:] = [x+size,y+size]\n",
    "        \n",
    "        \n",
    "        if np.all(points_in_poly(corners,np.column_stack((x_leaf_plus,y_leaf_plus)))) and i < n_leaf:\n",
    "            # leaf\n",
    "            # get the hog features of this cell\n",
    "            leaf[:,:,i] = image_stack[frames[f],x:x+size,y:y+size]\n",
    "            square = plt.Rectangle((x,y),size,size,color='r',fill=False)\n",
    "            plt.gca().add_patch(square)\n",
    "            i = i + 1\n",
    "        \n",
    "        \n",
    "        if np.all(points_in_poly(corners,np.column_stack((x_shell_plus,y_shell_plus)))) and np.all(~points_in_poly(corners,np.column_stack((x_leaf_minus,y_leaf_minus)))) and j < n_shell:\n",
    "            # shell\n",
    "            shell[:,:,j] = image_stack[frames[f],x:x+size,y:y+size]\n",
    "            square = plt.Rectangle((x,y),size,size,color='y',fill=False)\n",
    "            plt.gca().add_patch(square)\n",
    "            j = j + 1\n",
    "        \n",
    "        \n",
    "        if np.all(~points_in_poly(corners,np.column_stack((x_shell_plus,y_shell_plus)))) and k < n_background:\n",
    "            # background\n",
    "            background[:,:,k] = image_stack[frames[f],x:x+size,y:y+size]\n",
    "            square = plt.Rectangle((x,y),size,size,color='b',fill=False)\n",
    "            plt.gca().add_patch(square)\n",
    "            k = k + 1\n",
    "\n",
    "    tiles = np.append(tiles,np.concatenate((leaf,shell,background),axis=2),axis=2)\n",
    "    label = np.concatenate((np.zeros(n_leaf)+2,np.zeros(n_shell)+1,np.zeros(n_background)))\n",
    "    labels = np.append(labels,label)\n",
    "    \n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.xlim([0,np.shape(image_stack)[1]])\n",
    "    plt.ylim([0,np.shape(image_stack)[2]])\n",
    "    plt.imshow(image_stack[frames[f],:,:].T,cmap='Greys_r')\n",
    "    plt.savefig('imgs/shell_leaf_boundaries_'+d+'_'+str(frames[f])+'_size'+str(size)+'.png',dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "print np.shape(tiles)\n",
    "print np.shape(labels)\n",
    "print len(labels[labels == 2]),len(labels[labels == 1]),len(labels[labels == 0])\n",
    "\n",
    "file_name = 'data_files/training_data_'+d+'_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'.dat'\n",
    "f = open(file_name,'w')\n",
    "pickle.dump([tiles,labels],f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:32: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.20 % variance retained in 27 dimensions\n",
      "95.05 % variance retained in 68 dimensions\n",
      "99.00 % variance retained in 327 dimensions\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# do PCA\n",
    "\n",
    "'''\n",
    "    Performs the Principal Coponent analysis of the Matrix X\n",
    "    Matrix must be n * m dimensions\n",
    "    where n is # features\n",
    "    m is # examples\n",
    "    '''\n",
    "\n",
    "def PCA(X, varRetained = [0.95],filename = 'PCA_data.dat'):\n",
    "    \n",
    "    # Compute Covariance Matrix Sigma\n",
    "    (n, m) = X.shape\n",
    "    \n",
    "    Sigma = 1.0 / float(m) * np.dot(X, np.transpose(X))\n",
    "    # Compute eigenvectors and eigenvalues of Sigma\n",
    "    U, s, V = np.linalg.svd(Sigma)\n",
    "    \n",
    "    # compute the value k: number of minumum features that\n",
    "    # retains the given variance\n",
    "    s_tot = np.sum(s)\n",
    "    \n",
    "    var_i = np.array([np.sum(s[: i + 1]) / s_tot * 100.0 for i in range(n)])\n",
    "    \n",
    "    k = np.zeros(len(varRetained))\n",
    "    for i in range(len(k)):\n",
    "        k[i] = len(var_i[var_i < (varRetained[i] * 100e0)])\n",
    "        \n",
    "        print '%.2f %% variance retained in %d dimensions' % (var_i[k[i]], k[i])\n",
    "        \n",
    "        # compute the reduced dimensional features\n",
    "        U_reduced = U[:, : k[i]]\n",
    "        Z = np.dot(np.transpose(U_reduced),X)\n",
    "        \n",
    "        # pickle dump the results\n",
    "        f = open(filename+str(int(varRetained[i]*100e0))+'.dat','w')\n",
    "        pickle.dump([Z, U_reduced, k[i]],f)\n",
    "        f.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "var_ret = [0.90, 0.95, 0.99]\n",
    "\n",
    "# load the training data \n",
    "f = open('data_files/training_data_'+d+'_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'.dat','r')\n",
    "[tiles,labels] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "# standardize tiles\n",
    "tiles_standard = np.zeros(np.shape(tiles))\n",
    "\n",
    "mean = np.mean(image_stack)\n",
    "std = np.std(image_stack)\n",
    "for i in range(len(labels)):\n",
    "    tiles_standard[:,:,i] = (tiles[:,:,i] - mean) / std\n",
    "\n",
    "# reshape tiles\n",
    "reshape_tiles = np.reshape(tiles_standard,[size*size,len(labels)])\n",
    "\n",
    "filename = 'data_files/PCA_data_'+d+'_'+str(size)+'x'+str(size)+'_var0'\n",
    "\n",
    "# do PCA and save the results\n",
    "PCA(reshape_tiles,varRetained = var_ret,filename = filename)\n",
    "\n",
    "print 'finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.89279969114\n",
      "    {'n_jobs': 3, 'min_samples_leaf': 4, 'n_estimators': 79, 'max_features': 65, 'max_depth': 18, 'class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "# train a random forest with RandomizedSearchCV\n",
    "\n",
    "file_name = 'data_files/training_data_'+d+'_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'.dat'\n",
    "f = open(file_name,'r')\n",
    "[X,Y] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('data_files/PCA_data_'+d+'_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'.dat','r')\n",
    "[Z, U_reduced, k] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# initialize the classifier\n",
    "RF = RandomForestClassifier()\n",
    "\n",
    "# parameter grid for randomized search\n",
    "param_grid = {\"max_depth\": sp_randint(1, 20),\n",
    "          \"max_features\": sp_randint(1, np.shape(Z.T)[1]),\n",
    "          \"min_samples_leaf\": sp_randint(1, 100),\n",
    "          \"class_weight\": [\"auto\"],\n",
    "          \"n_jobs\": [32],\n",
    "          \"n_estimators\": sp_randint(10, 100)}\n",
    "\n",
    "cv = StratifiedKFold(Y,n_folds=k_fold,shuffle=True)\n",
    "\n",
    "# do the parameter search\n",
    "search_RF = RandomizedSearchCV(RF,param_grid,n_iter=n_iter,cv=cv).fit(Z.T,Y)\n",
    "print '   ',search_RF.best_score_\n",
    "print '   ',search_RF.best_params_\n",
    "\n",
    "# save the results\n",
    "f = open('data_files/RF_tile_'+d+'_niter'+str(n_iter)+'_kfold'+str(k_fold)+'_var0'+str(int(var*100e0))+'.dat','w')\n",
    "pickle.dump([search_RF.grid_scores_,search_RF.best_score_,search_RF.best_params_,search_RF.best_estimator_],f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sp_uniform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-784396a02fa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# parameter grid for randomized search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m param_grid = {\"max_depth\": sp_randint(1, 20),\n\u001b[0;32m---> 17\u001b[0;31m               \u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msp_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0e0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m               \u001b[0;34m\"nthread\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m               \u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'multi:softprob'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sp_uniform' is not defined"
     ]
    }
   ],
   "source": [
    "# train an XGBoost with RandomizedSearchCV\n",
    "\n",
    "file_name = 'data_files/training_data_'+d+'_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'.dat'\n",
    "f = open(file_name,'r')\n",
    "[X,Y] = pickle.load(f)\n",
    "f.close()  \n",
    "\n",
    "f = open('data_files/PCA_data_'+d+'_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'.dat','r')\n",
    "[Z, U_reduced, k] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# initialize the classifier\n",
    "GB = xgb.XGBClassifier()\n",
    "\n",
    "# parameter grid for randomized search\n",
    "param_grid = {\"max_depth\": sp_randint(1, 20),\n",
    "              \"learning_rate\": sp_uniform(loc=0e0,scale=1e0),\n",
    "              \"nthread\":[32],\n",
    "              \"objective\":['multi:softprob'],\n",
    "              \"n_estimators\": sp_randint(50, 200)}\n",
    "\n",
    "cv = StratifiedKFold(Y,n_folds=k_fold,shuffle=True)\n",
    "\n",
    "# do the parameter search\n",
    "search_GB = RandomizedSearchCV(GB,param_grid,n_iter=n_iter,cv=cv).fit(Z.T,Y)\n",
    "print '   ',search_GB.best_score_\n",
    "print '   ',search_GB.best_params_\n",
    "\n",
    "# save the results\n",
    "f = open('data_files/GB_tile_'+d+'_niter'+str(n_iter)+'_kfold'+str(k_fold)+'_var0'+str(int(var*100e0))+'.dat','w')\n",
    "pickle.dump([search_GB.grid_scores_,search_GB.best_score_,search_GB.best_params_,search_GB.best_estimator_],f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6c540dc492b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# do the parameter search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msearch_SVM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVM_rbf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'   '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearch_SVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'   '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearch_SVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    551\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 for train, test in cv)\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train an rbf SVM with GridSearchCV\n",
    "\n",
    "file_name = 'data_files/training_data_'+d+'_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'.dat'\n",
    "f = open(file_name,'r')\n",
    "[X,Y] = pickle.load(f)\n",
    "f.close()  \n",
    "\n",
    "f = open('data_files/PCA_data_'+d+'_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'.dat','r')\n",
    "[Z, U_reduced, k] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# initialize the classifier\n",
    "SVM_rbf = SVC()\n",
    "\n",
    "# parameter grid for randomized search\n",
    "param_grid = {\"C\": 10e0**(np.linspace(0e0,4e0,5)),\n",
    "              \"class_weight\": [\"auto\"],\n",
    "              \"gamma\": 10e0**(np.linspace(-4e0,0e0,5))}\n",
    "\n",
    "cv = StratifiedKFold(Y,n_folds=k_fold,shuffle=True)\n",
    "\n",
    "# do the parameter search\n",
    "search_SVM = GridSearchCV(SVM_rbf,param_grid,cv=cv).fit(Z.T,Y)\n",
    "print '   ',search_SVM.best_score_\n",
    "print '   ',search_SVM.best_params_\n",
    "\n",
    "# save the results\n",
    "f = open('data_files/SVM_rbf_'+d+'_tile_niter'+str(n_iter)+'_kfold'+str(k_fold)+'_var0'+str(int(var*100e0))+'.dat','w')\n",
    "pickle.dump([search_SVM.grid_scores_,search_SVM.best_score_,search_SVM.best_params_,search_SVM.best_estimator_],f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "#render the classes of all classifiers\n",
    "\n",
    "f = open('data_files/PCA_data_'+d+'_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'.dat','r')\n",
    "[Z, U_reduced, k] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('data_files/RF_tile_'+d+'_niter'+str(n_iter)+'_kfold'+str(k_fold)+'_var0'+str(int(var*100e0))+'.dat','r')\n",
    "[RF_grid_scores,RF_best_score,RF_best_params,RF] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('data_files/GB_tile_'+d+'_niter'+str(n_iter)+'_kfold'+str(k_fold)+'_var0'+str(int(var*100e0))+'.dat','r')\n",
    "[GB_grid_scores,GB_best_score,GB_best_params,GB] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('data_files/SVM_rbf_'+d+'_tile_niter'+str(n_iter)+'_kfold'+str(k_fold)+'_var0'+str(int(var*100e0))+'.dat','r')\n",
    "[SVM_rbf_grid_scores,SVM_rbf_best_score,SVM_rbf_best_params,SVM_rbf] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "#f = open('data_files/SVM_lin_tile_niter'+str(n_iter)+'_kfold'+str(k_fold)+'.dat','r')\n",
    "#[SVM_lin_grid_scores,SVM_lin_best_score,SVM_lin_best_params,SVM_lin] = pickle.load(f)\n",
    "#f.close()\n",
    "\n",
    "#f = open('data_files/SVM_poly_tile_niter'+str(n_iter)+'_kfold'+str(k_fold)+'.dat','r')\n",
    "#[SVM_poly_grid_scores,SVM_poly_best_score,SVM_poly_best_params,SVM_poly] = pickle.load(f)\n",
    "#f.close()\n",
    "\n",
    "#f = open('data_files/knearest_tile_niter'+str(n_iter)+'_kfold'+str(k_fold)+'.dat','r')\n",
    "#[knearest_grid_scores,knearest_best_score,knearest_best_params,knearest] = pickle.load(f)\n",
    "#f.close()\n",
    "\n",
    "windows = view_as_windows(image_stack[0,:,:],(size,size))\n",
    "\n",
    "f_range = np.arange(np.shape(image_stack)[0],step=step_size)\n",
    "i_range = np.arange(np.shape(windows)[0],step=step_size)\n",
    "j_range = np.arange(np.shape(windows)[1],step=step_size)\n",
    "\n",
    "class_stack_RF = np.zeros([len(f_range),len(i_range),len(j_range)]).astype(int)\n",
    "class_stack_SVM_rbf = np.zeros([len(f_range),len(i_range),len(j_range)]).astype(int)\n",
    "class_stack_GB = np.zeros([len(f_range),len(i_range),len(j_range)]).astype(int)\n",
    "#class_stack_SVM_lin = np.zeros([len(f_range),len(i_range),len(j_range)]).astype(int)\n",
    "#class_stack_SVM_poly = np.zeros([len(f_range),len(i_range),len(j_range)]).astype(int)\n",
    "#class_stack_knearest = np.zeros([len(f_range),len(i_range),len(j_range)]).astype(int)\n",
    "\n",
    "mean_im = np.mean(image_stack)\n",
    "std_im = np.std(image_stack)\n",
    "for f in range(len(f_range)):\n",
    "    if f%(100/step_size) == 0:\n",
    "        print f\n",
    "    \n",
    "    windows = view_as_windows(image_stack[f_range[f],:,:],(size,size))\n",
    "    \n",
    "    PCA_features = np.zeros([len(i_range),len(j_range),np.shape(Z)[0]])\n",
    "    # collect the PCA features\n",
    "    for i in range(len(i_range)):\n",
    "        for j in range(len(j_range)):\n",
    "            tile_standard = (windows[i_range[i],j_range[j]] - mean_im)/std_im\n",
    "            reshape_tile = tile_standard.reshape(size*size)\n",
    "            PCA_features[i,j,:] = np.dot(reshape_tile,U_reduced)\n",
    "\n",
    "    # predict the classes:\n",
    "    for i in range(len(i_range)):\n",
    "        class_stack_RF[f,i,:] = RF.predict(PCA_features[i,:,:])\n",
    "        class_stack_SVM_rbf[f,i,:] = SVM_rbf.predict(PCA_features[i,:,:])\n",
    "        class_stack_GB[f,i,:] = GB.predict(PCA_features[i,:,:])\n",
    "        #class_stack_SVM_lin[f,i,:] = SVM_lin.predict(PCA_features[i,:,:])\n",
    "        #class_stack_SVM_poly[f,i,:] = SVM_poly.predict(PCA_features[i,:,:])\n",
    "        #class_stack_knearest[f,i,:] = knearest.predict(PCA_features[i,:,:])\n",
    "    \n",
    "    \n",
    "    if f%(100/step_size) == 0:\n",
    "        \n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.axis('equal')\n",
    "        plt.imshow(image_stack[f_range[f],:,:],cmap='Greys_r')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(class_stack_RF[f,:,:])\n",
    "        plt.savefig('imgs/contours_RF_'+d+'_size'+str(size)+'_tile'+str(f_range[f])+'.png',dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "f = open('data_files/class_stacks_'+d+'_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'.dat','w')\n",
    "#pickle.dump([class_stack_RF,class_stack_SVM_rbf,class_stack_SVM_lin,class_stack_SVM_poly,class_stack_GB,class_stack_knearest],f)\n",
    "pickle.dump([class_stack_RF,class_stack_SVM_rbf,class_stack_GB],f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<open file 'data_files/class_stacks_sample17_batch3_IJ_stack_ok_50x50_var090.dat', mode 'r' at 0x100f1eed0>\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:49: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "[3L, 310L, 334L, 496L, 1L, 1L, 1L, 1L]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "f = open('data_files/class_stacks_'+d+'_'+str(size)+'x'+str(size)+'_var0'+str(int(var*100e0))+'.dat','r')\n",
    "#pickle.dump([class_stack_RF,class_stack_SVM_rbf,class_stack_SVM_lin,class_stack_SVM_poly,class_stack_GB,class_stack_knearest],f)\n",
    "print f\n",
    "\n",
    "[class_stack_RF,class_stack_SVM_rbf,class_stack_GB] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "class_stack_RF[class_stack_RF == 1] = 0\n",
    "class_stack_SVM_rbf[class_stack_SVM_rbf == 1] = 0\n",
    "class_stack_GB[class_stack_GB == 1] = 0\n",
    "\n",
    "# combine the class stacks\n",
    "#class_stack = class_stack_RF + class_stack_SVM_rbf + class_stack_GB\n",
    "\n",
    "#class_stack = class_stack_RF + class_stack_SVM_rbf \n",
    "#class_stack = class_stack_RF + class_stack_GB\n",
    "class_stack = class_stack_SVM_rbf + class_stack_GB\n",
    "\n",
    "\n",
    "class_stack[class_stack < 4] = 0\n",
    "class_stack[class_stack >= 4] = 1\n",
    "\n",
    "boundary = np.zeros(np.shape(image_stack))\n",
    "\n",
    "for f in range(np.shape(image_stack)[0]):\n",
    "    \n",
    "    if f%50 == 0:\n",
    "        print f\n",
    "    \n",
    "    smoothed = gaussian_filter(class_stack[f/step_size,:,:].astype(float),sigma=2)\n",
    "    \n",
    "    contours = find_contours(smoothed, 0.5)\n",
    "    \n",
    "    if len(contours) != 0:\n",
    "    \n",
    "        # calculate the area of the contours\n",
    "        area = np.zeros(len(contours))\n",
    "\n",
    "        for n, contour in enumerate(contours):\n",
    "            x = contour[:,0]\n",
    "            y = contour[:,1]\n",
    "            # copy the first point to the end of the contour list to close the loop\n",
    "            x = np.append(x,x[0])\n",
    "            y = np.append(y,y[0])\n",
    "            # use Green's theorem to calculate the area of the contour\n",
    "            area[n] = np.abs(np.sum(x[:-1]*y[1:]-y[:-1]*x[1:]))/2e0\n",
    "\n",
    "        # find contour with the largest area\n",
    "        cont = contours[np.where(area == np.max(area))[0]]\n",
    "    \n",
    "    pixels_in_cont = grid_points_in_poly(np.shape(image_stack[f,:,:]),cont*step_size+size/2e0)\n",
    "    \n",
    "    boundary[f,:,:] = image_stack[f,:,:]*pixels_in_cont\n",
    "    \n",
    "    if f%100 == 0:\n",
    "    \n",
    "        plt.axis('equal')\n",
    "        plt.xlim([0,np.shape(image_stack[f,:,:])[0]])\n",
    "        plt.ylim([0,np.shape(image_stack[f,:,:])[1]])\n",
    "        plt.imshow(boundary[f,:,:],cmap='Greys_r',vmin=0,vmax=66000)\n",
    "        plt.savefig('imgs/boundary_tiles_'+d+'_frame'+str(f).zfill(4)+'_size'+str(size)+'.jpg',dpi=100)\n",
    "        plt.close()\n",
    "    \n",
    "\n",
    "nim = NiftiImage(zoom(boundary,0.5))\n",
    "print nim.header['dim']\n",
    "nim.header['datatype'] == ncl.NIFTI_TYPE_FLOAT64\n",
    "nim.save('data_files/'+d+'_boundary_half_res_size'+str(size)+'.nii.gz')\n",
    "\n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "# load the segmented nifti image and make an animation with the leaf parts overplotted as a semi-transparent layer\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "from sklearn.svm import SVC\n",
    "from skimage.util.shape import view_as_windows\n",
    "import pickle\n",
    "from skimage.measure import grid_points_in_poly\n",
    "from skimage.measure import find_contours\n",
    "from skimage.filters import gaussian_filter\n",
    "from nifti import *\n",
    "import nifti.clib as ncl\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "\n",
    "nim = NiftiImage('data_files/segmented_sargentii_17_varret090_2combined.nii.gz').asarray()\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "colors = [(0.0, 0.0, 0.0, 0e0),  # black\n",
    "            (1.0, 0.0, 0.0, alpha)]  # red\n",
    "cmap = matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "for i in range(np.shape(image_stack)[0]):\n",
    "    if i%50 == 0:\n",
    "        print i\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.axis('equal')\n",
    "    plt.title('segmented leaf')\n",
    "    plt.xlabel('pixel coord.')\n",
    "    plt.ylabel('pixel coord.')\n",
    "    plt.imshow(image_stack[i,:,:],cmap='Greys_r',vmin=10000,vmax=33000)\n",
    "    plt.contourf(zoom(nim[i/2,:,:],2e0),cmap=cmap,antialiased=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('animation_border/segmented/segmented_size'+str(size)+'_frame'+str(i).zfill(4)+'.png',dpi=100)\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
