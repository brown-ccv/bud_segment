{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open and standardize image files, try stuff\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import ndimage\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = np.array(Image.open(files[0]))\n",
    "    \n",
    "    # define image_stack array\n",
    "    image_stack = np.zeros([len(files),np.shape(im)[0],np.shape(im)[1]])\n",
    "    # read and standardize all images\n",
    "    for i in range(len(files)):\n",
    "        im = np.array(Image.open(files[i]))        \n",
    "        \n",
    "        mean = np.mean(im)\n",
    "        std = np.std(im)\n",
    "        image_stack[i,:,:] = (im - mean)/std\n",
    "        \n",
    "        # for every nth images, explore\n",
    "        if i%100 == 0:\n",
    "            indx = files[i].index('.t')\n",
    "            \n",
    "            # histogram\n",
    "            #hist, bin_edges = np.histogram(im,bins=1000,range=(0,50000))\n",
    "            # to avoid log(0) later\n",
    "            #hist[hist < 1] == 0.1\n",
    "            #bins = (bin_edges[1:]+bin_edges[:-1])/2e0            \n",
    "            \n",
    "            #plt.close()\n",
    "            #width = (bin_edges[1] - bin_edges[0])\n",
    "            #center = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "            #plt.xlabel('pixel intensity')\n",
    "            #plt.ylabel('log(count)')\n",
    "            #plt.xlim([0,50000])\n",
    "            #plt.ylim([0,5])\n",
    "            #plt.bar(bin_edges[:-1], np.log10(hist), width)\n",
    "            #plt.tight_layout(w_pad=0, h_pad=0)\n",
    "            #plt.savefig('imgs/histo_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            #plt.close()\n",
    "            \n",
    "            # LBP\n",
    "            #lbp = local_binary_pattern(im, 8, 1)\n",
    "            #plt.imshow(lbp)\n",
    "            #plt.savefig('imgs/lbp_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            #plt.close()            \n",
    "\n",
    "            # k means clustering\n",
    "            clustered_img = KMeans(2).fit_predict(im.reshape([np.prod(np.shape(im)),1]))            \n",
    "            \n",
    "            clustered_img[clustered_img == clustered_img[0]] = 2e0\n",
    "            clustered_img[clustered_img != clustered_img[0]] = 3e0\n",
    "            \n",
    "            clustered_img = clustered_img.reshape(np.shape(im))\n",
    "            plt.imshow(clustered_img)\n",
    "            plt.savefig('imgs/kmeans_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # label continouos regions\n",
    "            labeled_img, num_regions = ndimage.label(clustered_img)\n",
    "            \n",
    "            plt.imshow(labeled_img)\n",
    "            plt.savefig('imgs/labeled_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # generate the size histogram of different regions\n",
    "            histo = np.zeros(num_regions)\n",
    "            for j in range(num_regions):\n",
    "                histo[j] = np.sum(labeled_img == j)\n",
    "            \n",
    "            histo[histo == 0] = 0.1\n",
    "            histo = np.sort(histo)[::-1]\n",
    "            \n",
    "            plt.xlabel('region label')\n",
    "            plt.ylabel('region size')\n",
    "            plt.xlim([0,20])\n",
    "            plt.ylim([0,6])\n",
    "            plt.bar(range(num_regions), np.log10(histo), np.zeros(num_regions)+1e0)\n",
    "            plt.tight_layout(w_pad=0, h_pad=0)\n",
    "            plt.savefig('imgs/area_histo_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sargentii_17 0\n",
      "sargentii_17 100\n",
      "sargentii_17 200\n",
      "sargentii_17 300\n",
      "sargentii_17 400\n",
      "sargentii_17 500\n",
      "sargentii_17 600\n",
      "sargentii_17 700\n",
      "sargentii_17 800\n",
      "sargentii_17 900\n"
     ]
    }
   ],
   "source": [
    "# pickle dump the clustered image_stack arrays so i don't need to always wait until all images are read and processes.\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "\n",
    "#dirs = ['plicatum_36','sargentii_17']\n",
    "dirs = ['sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = zoom(np.array(Image.open(files[0])),0.5)\n",
    "    \n",
    "    # define image_stack array\n",
    "    # use every second frame and downscale the image\n",
    "    image_stack = np.zeros([len(files)/2+1,np.shape(im)[0],np.shape(im)[1]],dtype='int')\n",
    "    # read images and cluster the pixels\n",
    "    for i in range(len(files)):\n",
    "        if i%100 == 0:\n",
    "            print d,i\n",
    "        \n",
    "        # fill up downscaled array\n",
    "        if i%2 == 0:\n",
    "            # load image\n",
    "            im = zoom(np.array(Image.open(files[i])),0.5)\n",
    "            # do clustering\n",
    "            clustered_img = KMeans(2).fit_predict(im.reshape([np.prod(np.shape(im)),1]))\n",
    "            # make sure the background is 0, the bud is 1, clustering randomizes which one is which\n",
    "            # assuming here that the corner pixel is always background\n",
    "            if clustered_img[0] != 0:\n",
    "                clustered_img ^= 1\n",
    "            # reshape array\n",
    "            clustered_img = clustered_img.reshape(np.shape(im))\n",
    "            # save stack\n",
    "            image_stack[i/2,:,:] = clustered_img\n",
    "\n",
    "    # dump image_stack array\n",
    "    f = open('data_files/'d+'.dat','w')\n",
    "    pickle.dump(image_stack,f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5c998ff23d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# try marching cubes to extract the surface of the bud\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mayavi import mlab\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "\n",
    "for d in dirs:\n",
    "    f = open('data_files/'+d+'.dat','r')\n",
    "    image_stack = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    zoomed = zoom(image_stack,0.25)\n",
    "\n",
    "    # march the cubes\n",
    "    verts, faces = measure.marching_cubes(zoomed, 0.5)\n",
    "    # GEOM view, visit (check on oscar) - for visualization\n",
    "    \n",
    "    # make the plot with matplotlib\n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    mesh = Poly3DCollection(verts[faces])\n",
    "    ax.add_collection3d(mesh)\n",
    "    ax.set_xlabel(\"x-axis\")\n",
    "    ax.set_ylabel(\"y-axis\")\n",
    "    ax.set_zlabel(\"z-axis\")\n",
    "    ax.set_xlim(0, np.shape(zoomed)[0])  \n",
    "    ax.set_ylim(0, np.shape(zoomed)[1])  \n",
    "    ax.set_zlim(0, np.shape(zoomed)[2]) \n",
    "    plt.savefig('imgs/marching_cubes_'+d+'_plt.png')\n",
    "    plt.close()\n",
    "    \n",
    "    for i in range(360):\n",
    "        # make the plot with mlab\n",
    "        mlab.triangular_mesh([vert[0] for vert in verts],\n",
    "                  [vert[1] for vert in verts],\n",
    "                  [vert[2] for vert in verts], faces) \n",
    "        mlab.view(azimuth = i)\n",
    "        mlab.savefig('ani_'+d+'/ani_'+str(i).zfill(3)+'.png')\n",
    "        mlab.close()\n",
    "        \n",
    "    # to convert the pngs into a movie use\n",
    "    # ffmpeg -framerate 25 -i ani_%03d.png -s:v 640x360 -c:v libx264 -profile:v high -crf 20 -pix_fmt yuv420p ani.mp4\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/ndimage/interpolation.py:549: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# for oscar: generate verts and faces and save the results in various resolutions\n",
    "# do not run on laptop\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    f = open('data_files/'+d+'.dat','r')\n",
    "    image_stack = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    ## march the cubes - full resolution\n",
    "    #verts, faces = measure.marching_cubes(image_stack, 0.5)\n",
    "    #f = open('data_files/verts_faces_'+d+'_full_res.dat','w')\n",
    "    #pickle.dump([verts, faces],f)\n",
    "    #f.close()\n",
    "\n",
    "    # march the cubes - downscale by a factor of 2\n",
    "    verts, faces = measure.marching_cubes(zoom(image_stack,0.5), 0.5)\n",
    "    f = open('data_files/verts_faces_'+d+'_half_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "\n",
    "    ## march the cubes - downscale by a factor of 4\n",
    "    #verts, faces = measure.marching_cubes(zoom(image_stack,0.25), 0.5)\n",
    "    #f = open('data_files/verts_faces_'+d+'_quarter_res.dat','w')\n",
    "    #pickle.dump([verts, faces],f)\n",
    "    #f.close()\n",
    "\n",
    "    ## march the cubes - downscale by a factor of 8\n",
    "    #verts, faces = measure.marching_cubes(zoom(image_stack,0.125), 0.5)\n",
    "    #f = open('data_files/verts_faces_'+d+'_octo_res.dat','w')\n",
    "    #pickle.dump([verts, faces],f)\n",
    "    #f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# work with the verts, faces\n",
    "# separate the unconnected surfaces, and clean the data by removing the small isolated surfaces (noise).\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "from mayavi import mlab\n",
    "\n",
    "\n",
    "dirs = ['sargentii_17','plicatum_36']\n",
    "\n",
    "res = 'half'\n",
    "\n",
    "def surface_finder(faces,surface,elements_removed):\n",
    "    unique_elements = np.unique(surface)\n",
    "    elements_to_check = np.setdiff1d(unique_elements,elements_removed,assume_unique=True)\n",
    "    if np.all([np.any(e not in faces) for e in elements_to_check]):\n",
    "        return faces, surface, elements_removed\n",
    "    else:\n",
    "        indcs = np.any([np.any(faces == e,axis=1) for e in elements_to_check],axis=0)\n",
    "        surf = faces[indcs,:]\n",
    "        new_surface = np.concatenate((surface,surf),axis=0)\n",
    "        new_faces = faces[~indcs,:]\n",
    "        elements_removed = np.concatenate((elements_removed,elements_to_check))\n",
    "        return surface_finder(new_faces,new_surface,elements_removed)\n",
    "\n",
    "for d in dirs:\n",
    "\n",
    "    f = open('data_files/verts_faces_'+d+'_'+res+'_res.dat','r')\n",
    "    [verts, faces] = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # verts are a list of 3D point coordinates.\n",
    "    # faces describe which 3 points form a triangle that should be shaded.\n",
    "    # e.g., this is one triangle:\n",
    "    # verts = [[  1.    3.   32.5]\n",
    "    # [  0.5   3.   33. ]\n",
    "    # [  1.    2.5  33. ]]\n",
    "    # faces = [[ 0  1  2]]\n",
    "    # point 0 is [  1.    3.   32.5]\n",
    "    # point 1 is [  0.5   3.   33. ]\n",
    "    # point 2 is [  1.    2.5  33. ]\n",
    "    # faces [ 0  1  2] describes that these three points form a triangle.\n",
    "    #\n",
    "    # separate surfaces do not have common points.\n",
    "    \n",
    "    print np.shape(verts)\n",
    "    print np.shape(faces)\n",
    "    \n",
    "    # find separate surfaces\n",
    "    surfaces = []\n",
    "    \n",
    "    while np.shape(faces)[0] > 0:\n",
    "        print 'new surface',np.shape(faces)[0]\n",
    "        surface = [faces[0,:]]\n",
    "        faces = np.delete(faces,0,axis=0)    \n",
    "        elements_removed = np.empty(0,dtype=int)\n",
    "\n",
    "        faces, surface, elements_removed = surface_finder(faces,surface,elements_removed)\n",
    "        unique_elements = np.unique(surface)\n",
    "        if np.any([np.any(e in faces) for e in unique_elements]):\n",
    "            print 'there are points left in faces that should be in surfaces'\n",
    "            print unique_elements\n",
    "            print np.shape(surface),np.shape(faces)\n",
    "            raise ValueError\n",
    "        surfaces.append(surface)\n",
    "    \n",
    "    f = open('data_files/surfaces_'+d+'_'+res+'_res.dat','w')\n",
    "    pickle.dump(surfaces,f)\n",
    "    f.close()\n",
    "    \n",
    "    surf_size = np.zeros(len(surfaces))\n",
    "    for i in range(len(surfaces)):\n",
    "        surf_size[i] = np.shape(surfaces[i])[0]\n",
    "    \n",
    "    print surf_size\n",
    "    stop\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(486378, 3)\n",
      "(664342, 3)\n"
     ]
    }
   ],
   "source": [
    "# make the movie of various surfaces\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "from mayavi import mlab\n",
    "\n",
    "dirs = ['sargentii_17','plicatum_36',]\n",
    "\n",
    "for d in dirs:\n",
    "\n",
    "    f = open('data_files/verts_faces_'+d+'_'+res+'_res.dat','r')\n",
    "    [verts, faces] = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    f = open('data_files/surfaces_'+d+'_'+res+'_res.dat','r')\n",
    "    surfaces = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    surf_size = np.zeros(len(surfaces))\n",
    "    for i in range(len(surfaces)):\n",
    "        surf_size[i] = np.shape(surfaces[i])[0]\n",
    "    \n",
    "    \n",
    "    sorted_indcs = np.argsort(surf_size)\n",
    "        \n",
    "    # make the plot with mlab\n",
    "    for i in range(9):\n",
    "        mlab.triangular_mesh([vert[0] for vert in verts],\n",
    "              [vert[1] for vert in verts],\n",
    "              [vert[2] for vert in verts], surfaces[sorted_indcs[-1]]) \n",
    "        mlab.view(azimuth = i*40)\n",
    "        mlab.savefig('ani_'+d+'/ani_'+str(i).zfill(3)+'.png')\n",
    "        mlab.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
