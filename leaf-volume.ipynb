{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open and standardize image files, try stuff\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import ndimage\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = np.array(Image.open(files[0]))\n",
    "    \n",
    "    # define image_stack array\n",
    "    image_stack = np.zeros([len(files),np.shape(im)[0],np.shape(im)[1]])\n",
    "    # read and standardize all images\n",
    "    for i in range(len(files)):\n",
    "        im = np.array(Image.open(files[i]))        \n",
    "        \n",
    "        mean = np.mean(im)\n",
    "        std = np.std(im)\n",
    "        image_stack[i,:,:] = (im - mean)/std\n",
    "        \n",
    "        # for every nth images, explore\n",
    "        if i%100 == 0:\n",
    "            indx = files[i].index('.t')\n",
    "            \n",
    "            # histogram\n",
    "            #hist, bin_edges = np.histogram(im,bins=1000,range=(0,50000))\n",
    "            # to avoid log(0) later\n",
    "            #hist[hist < 1] == 0.1\n",
    "            #bins = (bin_edges[1:]+bin_edges[:-1])/2e0            \n",
    "            \n",
    "            #plt.close()\n",
    "            #width = (bin_edges[1] - bin_edges[0])\n",
    "            #center = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "            #plt.xlabel('pixel intensity')\n",
    "            #plt.ylabel('log(count)')\n",
    "            #plt.xlim([0,50000])\n",
    "            #plt.ylim([0,5])\n",
    "            #plt.bar(bin_edges[:-1], np.log10(hist), width)\n",
    "            #plt.tight_layout(w_pad=0, h_pad=0)\n",
    "            #plt.savefig('imgs/histo_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            #plt.close()\n",
    "            \n",
    "            # LBP\n",
    "            #lbp = local_binary_pattern(im, 8, 1)\n",
    "            #plt.imshow(lbp)\n",
    "            #plt.savefig('imgs/lbp_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            #plt.close()            \n",
    "\n",
    "            # k means clustering\n",
    "            clustered_img = KMeans(2).fit_predict(im.reshape([np.prod(np.shape(im)),1]))            \n",
    "            \n",
    "            clustered_img[clustered_img == clustered_img[0]] = 2e0\n",
    "            clustered_img[clustered_img != clustered_img[0]] = 3e0\n",
    "            \n",
    "            clustered_img = clustered_img.reshape(np.shape(im))\n",
    "            plt.imshow(clustered_img)\n",
    "            plt.savefig('imgs/kmeans_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # label continouos regions\n",
    "            labeled_img, num_regions = ndimage.label(clustered_img)\n",
    "            \n",
    "            plt.imshow(labeled_img)\n",
    "            plt.savefig('imgs/labeled_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # generate the size histogram of different regions\n",
    "            histo = np.zeros(num_regions)\n",
    "            for j in range(num_regions):\n",
    "                histo[j] = np.sum(labeled_img == j)\n",
    "            \n",
    "            histo[histo == 0] = 0.1\n",
    "            histo = np.sort(histo)[::-1]\n",
    "            \n",
    "            plt.xlabel('region label')\n",
    "            plt.ylabel('region size')\n",
    "            plt.xlim([0,20])\n",
    "            plt.ylim([0,6])\n",
    "            plt.bar(range(num_regions), np.log10(histo), np.zeros(num_regions)+1e0)\n",
    "            plt.tight_layout(w_pad=0, h_pad=0)\n",
    "            plt.savefig('imgs/area_histo_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sargentii_17 0\n",
      "sargentii_17 100\n",
      "sargentii_17 200\n",
      "sargentii_17 300\n",
      "sargentii_17 400\n",
      "sargentii_17 500\n",
      "sargentii_17 600\n",
      "sargentii_17 700\n",
      "sargentii_17 800\n",
      "sargentii_17 900\n"
     ]
    }
   ],
   "source": [
    "# pickle dump the clustered image_stack arrays so i don't need to always wait until all images are read and processes.\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "\n",
    "#dirs = ['plicatum_36','sargentii_17']\n",
    "dirs = ['sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = zoom(np.array(Image.open(files[0])),0.5)\n",
    "    \n",
    "    # define image_stack array\n",
    "    # use every second frame and downscale the image\n",
    "    image_stack = np.zeros([len(files)/2+1,np.shape(im)[0],np.shape(im)[1]],dtype='int')\n",
    "    # read images and cluster the pixels\n",
    "    for i in range(len(files)):\n",
    "        if i%100 == 0:\n",
    "            print d,i\n",
    "        \n",
    "        # fill up downscaled array\n",
    "        if i%2 == 0:\n",
    "            # load image\n",
    "            im = zoom(np.array(Image.open(files[i])),0.5)\n",
    "            # do clustering\n",
    "            clustered_img = KMeans(2).fit_predict(im.reshape([np.prod(np.shape(im)),1]))\n",
    "            # make sure the background is 0, the bud is 1, clustering randomizes which one is which\n",
    "            # assuming here that the corner pixel is always background\n",
    "            if clustered_img[0] != 0:\n",
    "                clustered_img ^= 1\n",
    "            # reshape array\n",
    "            clustered_img = clustered_img.reshape(np.shape(im))\n",
    "            # save stack\n",
    "            image_stack[i/2,:,:] = clustered_img\n",
    "\n",
    "    # dump image_stack array\n",
    "    f = open(d+'.dat','w')\n",
    "    pickle.dump(image_stack,f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5c998ff23d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# try marching cubes to extract the surface of the bud\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mayavi import mlab\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "\n",
    "for d in dirs:\n",
    "    f = open(d+'.dat','r')\n",
    "    image_stack = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    zoomed = zoom(image_stack,0.25)\n",
    "\n",
    "    # march the cubes\n",
    "    verts, faces = measure.marching_cubes(zoomed, 0.5)\n",
    "    # GEOM view, visit (check on oscar) - for visualization\n",
    "    \n",
    "    # make the plot with matplotlib\n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    mesh = Poly3DCollection(verts[faces])\n",
    "    ax.add_collection3d(mesh)\n",
    "    ax.set_xlabel(\"x-axis\")\n",
    "    ax.set_ylabel(\"y-axis\")\n",
    "    ax.set_zlabel(\"z-axis\")\n",
    "    ax.set_xlim(0, np.shape(zoomed)[0])  \n",
    "    ax.set_ylim(0, np.shape(zoomed)[1])  \n",
    "    ax.set_zlim(0, np.shape(zoomed)[2]) \n",
    "    plt.savefig('imgs/marching_cubes_'+d+'_plt.png')\n",
    "    plt.close()\n",
    "    \n",
    "    for i in range(360):\n",
    "        # make the plot with mlab\n",
    "        mlab.triangular_mesh([vert[0] for vert in verts],\n",
    "                  [vert[1] for vert in verts],\n",
    "                  [vert[2] for vert in verts], faces) \n",
    "        mlab.view(azimuth = i)\n",
    "        mlab.savefig('ani_'+d+'/ani_'+str(i).zfill(3)+'.png')\n",
    "        mlab.close()\n",
    "        \n",
    "    # to convert the pngs into a movie use\n",
    "    # ffmpeg -framerate 25 -i ani_%03d.png -s:v 640x360 -c:v libx264 -profile:v high -crf 20 -pix_fmt yuv420p ani.mp4\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for oscar: generate verts and faces and save the results in various resolutions\n",
    "# do not run on laptop\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = np.array(Image.open(files[0]))\n",
    "    \n",
    "    # define image_stack array\n",
    "    # use every second frame and downscale the image\n",
    "    image_stack = np.zeros([len(files),np.shape(im)[0],np.shape(im)[1]],dtype='int')\n",
    "    # read images and cluster the pixels\n",
    "    for i in range(len(files)):\n",
    "        if i%100 == 0:\n",
    "            print d,i\n",
    "        \n",
    "        # fill up array\n",
    "        # load image\n",
    "        im = np.array(Image.open(files[i]))\n",
    "        # do clustering\n",
    "        clustered_img = KMeans(2).fit_predict(im.reshape([np.prod(np.shape(im)),1]))\n",
    "        # make sure the background is 0, the bud is 1, clustering randomizes which one is which\n",
    "        # assuming here that the corner pixel is always background\n",
    "        if clustered_img[0] != 0:\n",
    "            clustered_img ^= 1\n",
    "        # reshape array\n",
    "        clustered_img = clustered_img.reshape(np.shape(im))\n",
    "        # save stack\n",
    "        image_stack[i,:,:] = clustered_img\n",
    "\n",
    "    # march the cubes - full resolution\n",
    "    verts, faces = measure.marching_cubes(image_stack, 0.5)\n",
    "    f = open('verts_faces_'+d+'_full_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "\n",
    "    # march the cubes - downscale by a factor of 2\n",
    "    verts, faces = measure.marching_cubes(zoom(image_stack,0.5), 0.5)\n",
    "    f = open('verts_faces_'+d+'_half_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "\n",
    "    # march the cubes - downscale by a factor of 4\n",
    "    verts, faces = measure.marching_cubes(zoom(image_stack,0.25), 0.5)\n",
    "    f = open('verts_faces_'+d+'_quarter_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "\n",
    "    # march the cubes - downscale by a factor of 8\n",
    "    verts, faces = measure.marching_cubes(zoom(image_stack,0.125), 0.5)\n",
    "    f = open('verts_faces_'+d+'_octo_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483922, 3)\n",
      "(924912, 3)\n",
      "first line\n",
      "jumped here\n",
      "(9, 3) (924903, 3)\n",
      "first line\n",
      "jumped here\n",
      "(19, 3) (924893, 3)\n",
      "first line\n",
      "jumped here\n",
      "(28, 3) (924884, 3)\n",
      "first line\n",
      "jumped here\n",
      "(36, 3) (924876, 3)\n",
      "first line\n",
      "jumped here\n",
      "(44, 3) (924868, 3)\n",
      "first line\n",
      "jumped here\n",
      "(52, 3) (924860, 3)\n",
      "first line\n",
      "jumped here\n",
      "(56, 3) (924856, 3)\n",
      "first line\n",
      "[   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14\n",
      "   15   26   27   28   29   30   31   32 6456 6457 6458 6459 6460 6461 6462]\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "(56, 3) (924856, 3)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-beb51e5786ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_surface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_faces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# work with the verts, faces\n",
    "# separate the unconnected surfaces, and clean the data by removing the small isolated surfaces (noise).\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "from mayavi import mlab\n",
    "import itertools\n",
    "\n",
    "dirs = ['sargentii_17','plicatum_36']\n",
    "\n",
    "for d in dirs:\n",
    "\n",
    "    f = open('verts_faces_'+d+'_octo_res.dat','r')\n",
    "    [verts, faces] = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # verts are a list of 3D point coordinates.\n",
    "    # faces describe which 3 points form a triangle that should be shaded.\n",
    "    # e.g., this is one triangle:\n",
    "    # verts = [[  1.    3.   32.5]\n",
    "    # [  0.5   3.   33. ]\n",
    "    # [  1.    2.5  33. ]]\n",
    "    # faces = [[ 0  1  2]]\n",
    "    # point 0 is [  1.    3.   32.5]\n",
    "    # point 1 is [  0.5   3.   33. ]\n",
    "    # point 2 is [  1.    2.5  33. ]\n",
    "    # faces [ 0  1  2] describes that these three points form a triangle.\n",
    "    # The boundary between two unconnected surfaces are where two faces do not have common points.\n",
    "    # e.g., there is a surface boundary in the middle here:\n",
    "    # [22 21 23]\n",
    "    # [22 24 23]\n",
    "    # [25 24 23] ____ boundary\n",
    "    # [26  1  0]\n",
    "    # [ 1 26  4]\n",
    "    # [26 27  4]\n",
    "    \n",
    "    print np.shape(verts)\n",
    "    print np.shape(faces)\n",
    "    \n",
    "    # find surfaces\n",
    "    surface = [faces[0,:]]\n",
    "    faces = np.delete(faces,0,axis=0)    \n",
    "\n",
    "    def surface_finder(faces,surface):\n",
    "        print 'first line'\n",
    "        unique_elements = np.unique(surface)\n",
    "        if np.all([np.any(e not in faces) for e in unique_elements]):\n",
    "            return faces, surface\n",
    "        else:\n",
    "            print 'jumped here'\n",
    "            indcs = np.any([np.any(faces == e,axis=1) for e in unique_elements],axis=0)\n",
    "            surf = faces[indcs,:]\n",
    "            new_surface = np.concatenate((surface,surf),axis=0)\n",
    "            new_faces = faces[~indcs,:]\n",
    "            print np.shape(new_surface),np.shape(new_faces)\n",
    "            \n",
    "            return surface_finder(new_faces,new_surface)\n",
    "            \n",
    "\n",
    "        \n",
    "    new_faces, new_surface = surface_finder(faces,surface)\n",
    "    unique_elements = np.unique(new_surface)\n",
    "    print unique_elements\n",
    "    print [np.any(e in new_faces) for e in unique_elements]\n",
    "    \n",
    "    print np.shape(new_surface),np.shape(new_faces)\n",
    "    \n",
    "    stop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
