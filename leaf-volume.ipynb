{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open and standardize image files, try stuff\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import ndimage\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = np.array(Image.open(files[0]))\n",
    "    \n",
    "    # define image_stack array\n",
    "    image_stack = np.zeros([len(files),np.shape(im)[0],np.shape(im)[1]])\n",
    "    # read and standardize all images\n",
    "    for i in range(len(files)):\n",
    "        im = np.array(Image.open(files[i]))        \n",
    "        \n",
    "        mean = np.mean(im)\n",
    "        std = np.std(im)\n",
    "        image_stack[i,:,:] = (im - mean)/std\n",
    "        \n",
    "        # for every nth images, explore\n",
    "        if i%100 == 0:\n",
    "            indx = files[i].index('.t')\n",
    "            \n",
    "            # histogram\n",
    "            #hist, bin_edges = np.histogram(im,bins=1000,range=(0,50000))\n",
    "            # to avoid log(0) later\n",
    "            #hist[hist < 1] == 0.1\n",
    "            #bins = (bin_edges[1:]+bin_edges[:-1])/2e0            \n",
    "            \n",
    "            #plt.close()\n",
    "            #width = (bin_edges[1] - bin_edges[0])\n",
    "            #center = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "            #plt.xlabel('pixel intensity')\n",
    "            #plt.ylabel('log(count)')\n",
    "            #plt.xlim([0,50000])\n",
    "            #plt.ylim([0,5])\n",
    "            #plt.bar(bin_edges[:-1], np.log10(hist), width)\n",
    "            #plt.tight_layout(w_pad=0, h_pad=0)\n",
    "            #plt.savefig('imgs/histo_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            #plt.close()\n",
    "            \n",
    "            # LBP\n",
    "            #lbp = local_binary_pattern(im, 8, 1)\n",
    "            #plt.imshow(lbp)\n",
    "            #plt.savefig('imgs/lbp_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            #plt.close()            \n",
    "\n",
    "            # k means clustering\n",
    "            clustered_img = KMeans(2).fit_predict(im.reshape([np.prod(np.shape(im)),1]))            \n",
    "            \n",
    "            clustered_img[clustered_img == clustered_img[0]] = 2e0\n",
    "            clustered_img[clustered_img != clustered_img[0]] = 3e0\n",
    "            \n",
    "            clustered_img = clustered_img.reshape(np.shape(im))\n",
    "            plt.imshow(clustered_img)\n",
    "            plt.savefig('imgs/kmeans_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # label continouos regions\n",
    "            labeled_img, num_regions = ndimage.label(clustered_img)\n",
    "            \n",
    "            plt.imshow(labeled_img)\n",
    "            plt.savefig('imgs/labeled_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # generate the size histogram of different regions\n",
    "            histo = np.zeros(num_regions)\n",
    "            for j in range(num_regions):\n",
    "                histo[j] = np.sum(labeled_img == j)\n",
    "            \n",
    "            histo[histo == 0] = 0.1\n",
    "            histo = np.sort(histo)[::-1]\n",
    "            \n",
    "            plt.xlabel('region label')\n",
    "            plt.ylabel('region size')\n",
    "            plt.xlim([0,20])\n",
    "            plt.ylim([0,6])\n",
    "            plt.bar(range(num_regions), np.log10(histo), np.zeros(num_regions)+1e0)\n",
    "            plt.tight_layout(w_pad=0, h_pad=0)\n",
    "            plt.savefig('imgs/area_histo_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sargentii_17 0\n",
      "sargentii_17 100\n",
      "sargentii_17 200\n",
      "sargentii_17 300\n",
      "sargentii_17 400\n",
      "sargentii_17 500\n",
      "sargentii_17 600\n",
      "sargentii_17 700\n",
      "sargentii_17 800\n",
      "sargentii_17 900\n"
     ]
    }
   ],
   "source": [
    "# pickle dump the clustered image_stack arrays so i don't need to always wait until all images are read and processes.\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "\n",
    "#dirs = ['plicatum_36','sargentii_17']\n",
    "dirs = ['sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = zoom(np.array(Image.open(files[0])),0.5)\n",
    "    \n",
    "    # define image_stack array\n",
    "    # use every second frame and downscale the image\n",
    "    image_stack = np.zeros([len(files)/2+1,np.shape(im)[0],np.shape(im)[1]],dtype='int')\n",
    "    # read images and cluster the pixels\n",
    "    for i in range(len(files)):\n",
    "        if i%100 == 0:\n",
    "            print d,i\n",
    "        \n",
    "        # fill up downscaled array\n",
    "        if i%2 == 0:\n",
    "            # load image\n",
    "            im = zoom(np.array(Image.open(files[i])),0.5)\n",
    "            # do clustering\n",
    "            clustered_img = KMeans(2).fit_predict(im.reshape([np.prod(np.shape(im)),1]))\n",
    "            # make sure the background is 0, the bud is 1, clustering randomizes which one is which\n",
    "            # assuming here that the corner pixel is always background\n",
    "            if clustered_img[0] != 0:\n",
    "                clustered_img ^= 1\n",
    "            # reshape array\n",
    "            clustered_img = clustered_img.reshape(np.shape(im))\n",
    "            # save stack\n",
    "            image_stack[i/2,:,:] = clustered_img\n",
    "\n",
    "    # dump image_stack array\n",
    "    f = open(d+'.dat','w')\n",
    "    pickle.dump(image_stack,f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5c998ff23d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# try marching cubes to extract the surface of the bud\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mayavi import mlab\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "\n",
    "for d in dirs:\n",
    "    f = open(d+'.dat','r')\n",
    "    image_stack = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    zoomed = zoom(image_stack,0.25)\n",
    "\n",
    "    # march the cubes\n",
    "    verts, faces = measure.marching_cubes(zoomed, 0.5)\n",
    "    # GEOM view, visit (check on oscar) - for visualization\n",
    "    \n",
    "    # make the plot with matplotlib\n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    mesh = Poly3DCollection(verts[faces])\n",
    "    ax.add_collection3d(mesh)\n",
    "    ax.set_xlabel(\"x-axis\")\n",
    "    ax.set_ylabel(\"y-axis\")\n",
    "    ax.set_zlabel(\"z-axis\")\n",
    "    ax.set_xlim(0, np.shape(zoomed)[0])  \n",
    "    ax.set_ylim(0, np.shape(zoomed)[1])  \n",
    "    ax.set_zlim(0, np.shape(zoomed)[2]) \n",
    "    plt.savefig('imgs/marching_cubes_'+d+'_plt.png')\n",
    "    plt.close()\n",
    "    \n",
    "    for i in range(360):\n",
    "        # make the plot with mlab\n",
    "        mlab.triangular_mesh([vert[0] for vert in verts],\n",
    "                  [vert[1] for vert in verts],\n",
    "                  [vert[2] for vert in verts], faces) \n",
    "        mlab.view(azimuth = i)\n",
    "        mlab.savefig('ani_'+d+'/ani_'+str(i).zfill(3)+'.png')\n",
    "        mlab.close()\n",
    "        \n",
    "    # to convert the pngs into a movie use\n",
    "    # ffmpeg -framerate 25 -i ani_%03d.png -s:v 640x360 -c:v libx264 -profile:v high -crf 20 -pix_fmt yuv420p ani.mp4\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for oscar: generate verts and faces and save the results in various resolutions\n",
    "# do not run on laptop\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = np.array(Image.open(files[0]))\n",
    "    \n",
    "    # define image_stack array\n",
    "    # use every second frame and downscale the image\n",
    "    image_stack = np.zeros([len(files),np.shape(im)[0],np.shape(im)[1]],dtype='int')\n",
    "    # read images and cluster the pixels\n",
    "    for i in range(len(files)):\n",
    "        if i%100 == 0:\n",
    "            print d,i\n",
    "        \n",
    "        # fill up array\n",
    "        # load image\n",
    "        im = np.array(Image.open(files[i]))\n",
    "        # do clustering\n",
    "        clustered_img = KMeans(2).fit_predict(im.reshape([np.prod(np.shape(im)),1]))\n",
    "        # make sure the background is 0, the bud is 1, clustering randomizes which one is which\n",
    "        # assuming here that the corner pixel is always background\n",
    "        if clustered_img[0] != 0:\n",
    "            clustered_img ^= 1\n",
    "        # reshape array\n",
    "        clustered_img = clustered_img.reshape(np.shape(im))\n",
    "        # save stack\n",
    "        image_stack[i,:,:] = clustered_img\n",
    "\n",
    "    # march the cubes - full resolution\n",
    "    verts, faces = measure.marching_cubes(image_stack, 0.5)\n",
    "    f = open('verts_faces_'+d+'_full_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "\n",
    "    # march the cubes - downscale by a factor of 2\n",
    "    verts, faces = measure.marching_cubes(zoom(image_stack,0.5), 0.5)\n",
    "    f = open('verts_faces_'+d+'_half_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "\n",
    "    # march the cubes - downscale by a factor of 4\n",
    "    verts, faces = measure.marching_cubes(zoom(image_stack,0.25), 0.5)\n",
    "    f = open('verts_faces_'+d+'_quarter_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "\n",
    "    # march the cubes - downscale by a factor of 8\n",
    "    verts, faces = measure.marching_cubes(zoom(image_stack,0.125), 0.5)\n",
    "    f = open('verts_faces_'+d+'_octo_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483922, 3)\n",
      "(924912, 3)\n",
      "new surface 924912\n",
      "    3 new elements to check\n",
      "    (9, 3) (924903, 3)\n",
      "    5 new elements to check\n",
      "    (19, 3) (924893, 3)\n",
      "    5 new elements to check\n",
      "    (28, 3) (924884, 3)\n",
      "    4 new elements to check\n",
      "    (36, 3) (924876, 3)\n",
      "    4 new elements to check\n",
      "    (44, 3) (924868, 3)\n",
      "    4 new elements to check\n",
      "    (52, 3) (924860, 3)\n",
      "    4 new elements to check\n",
      "    (56, 3) (924856, 3)\n",
      "    1 new elements to check\n",
      "new surface 924856\n",
      "    3 new elements to check\n",
      "    (11, 3) (924845, 3)\n",
      "    7 new elements to check\n",
      "    (30, 3) (924826, 3)\n",
      "    14 new elements to check\n",
      "    (67, 3) (924789, 3)\n",
      "    23 new elements to check\n",
      "    (120, 3) (924736, 3)\n",
      "    34 new elements to check\n",
      "    (193, 3) (924663, 3)\n",
      "    44 new elements to check\n",
      "    (280, 3) (924576, 3)\n",
      "    52 new elements to check\n",
      "    (389, 3) (924467, 3)\n",
      "    53 new elements to check\n",
      "    (486, 3) (924370, 3)\n",
      "    49 new elements to check\n",
      "    (593, 3) (924263, 3)\n",
      "    60 new elements to check\n",
      "    (719, 3) (924137, 3)\n",
      "    68 new elements to check\n",
      "    (869, 3) (923987, 3)\n",
      "    86 new elements to check\n",
      "    (1060, 3) (923796, 3)\n",
      "    110 new elements to check\n",
      "    (1285, 3) (923571, 3)\n",
      "    126 new elements to check\n",
      "    (1555, 3) (923301, 3)\n",
      "    157 new elements to check\n",
      "    (1937, 3) (922919, 3)\n",
      "    238 new elements to check\n",
      "    (2421, 3) (922435, 3)\n",
      "    225 new elements to check\n",
      "    (2824, 3) (922032, 3)\n",
      "    205 new elements to check\n",
      "    (3232, 3) (921624, 3)\n",
      "    228 new elements to check\n",
      "    (3707, 3) (921149, 3)\n",
      "    261 new elements to check\n",
      "    (4228, 3) (920628, 3)\n",
      "    291 new elements to check\n",
      "    (4866, 3) (919990, 3)\n",
      "    371 new elements to check\n",
      "    (5627, 3) (919229, 3)\n",
      "    400 new elements to check\n",
      "    (6441, 3) (918415, 3)\n",
      "    453 new elements to check\n",
      "    (7374, 3) (917482, 3)\n",
      "    505 new elements to check\n",
      "    (8456, 3) (916400, 3)\n",
      "    595 new elements to check\n",
      "    (9638, 3) (915218, 3)\n",
      "    622 new elements to check\n",
      "   "
     ]
    }
   ],
   "source": [
    "# work with the verts, faces\n",
    "# separate the unconnected surfaces, and clean the data by removing the small isolated surfaces (noise).\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "from mayavi import mlab\n",
    "import itertools\n",
    "\n",
    "dirs = ['sargentii_17','plicatum_36']\n",
    "\n",
    "def surface_finder(faces,surface,elements_removed):\n",
    "    unique_elements = np.unique(surface)\n",
    "    elements_to_check = np.setdiff1d(unique_elements,elements_removed,assume_unique=True)\n",
    "    print '   ',len(elements_to_check),'new elements to check'\n",
    "    if np.all([np.any(e not in faces) for e in elements_to_check]):\n",
    "        return faces, surface, elements_removed\n",
    "    else:\n",
    "        indcs = np.any([np.any(faces == e,axis=1) for e in elements_to_check],axis=0)\n",
    "        surf = faces[indcs,:]\n",
    "        new_surface = np.concatenate((surface,surf),axis=0)\n",
    "        new_faces = faces[~indcs,:]\n",
    "        print '   ',np.shape(new_surface),np.shape(new_faces)\n",
    "        elements_removed = np.concatenate((elements_removed,elements_to_check))\n",
    "        return surface_finder(new_faces,new_surface,elements_removed)\n",
    "\n",
    "for d in dirs:\n",
    "\n",
    "    f = open('verts_faces_'+d+'_octo_res.dat','r')\n",
    "    [verts, faces] = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # verts are a list of 3D point coordinates.\n",
    "    # faces describe which 3 points form a triangle that should be shaded.\n",
    "    # e.g., this is one triangle:\n",
    "    # verts = [[  1.    3.   32.5]\n",
    "    # [  0.5   3.   33. ]\n",
    "    # [  1.    2.5  33. ]]\n",
    "    # faces = [[ 0  1  2]]\n",
    "    # point 0 is [  1.    3.   32.5]\n",
    "    # point 1 is [  0.5   3.   33. ]\n",
    "    # point 2 is [  1.    2.5  33. ]\n",
    "    # faces [ 0  1  2] describes that these three points form a triangle.\n",
    "    # two separate surfaces do not have common points.\n",
    "    \n",
    "    print np.shape(verts)\n",
    "    print np.shape(faces)\n",
    "    \n",
    "    # find surfaces\n",
    "    surfaces = []\n",
    "    \n",
    "    while np.shape(faces)[0] > 0:\n",
    "        print 'new surface',np.shape(faces)[0]\n",
    "        surface = [faces[0,:]]\n",
    "        faces = np.delete(faces,0,axis=0)    \n",
    "        elements_removed = np.empty(0,dtype=int)\n",
    "\n",
    "        faces, surface, elements_removed = surface_finder(faces,surface,elements_removed)\n",
    "        unique_elements = np.unique(surface)\n",
    "        if np.any([np.any(e in faces) for e in unique_elements]):\n",
    "            print 'there are points left in faces that should be in surfaces'\n",
    "            print unique_elements\n",
    "            print np.shape(surface),np.shape(faces)\n",
    "            raise ValueError\n",
    "        surfaces.append(surface)\n",
    "    \n",
    "    f = open('surfaces_'+d+'_octo_res.dat','w')\n",
    "    pickle.dump(surfaces,f)\n",
    "    f.close()\n",
    "    \n",
    "    surf_size = np.zeros(len(surfaces))\n",
    "    for i in range(len(surfaces)):\n",
    "        surf_size[i] = np.shape(surfaces[i])[0]\n",
    "    \n",
    "    print surf_size\n",
    "        \n",
    "        \n",
    "    stop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
