{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open and standardize image files, try stuff\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import ndimage\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = np.array(Image.open(files[0]))\n",
    "    \n",
    "    # define image_stack array\n",
    "    image_stack = np.zeros([len(files),np.shape(im)[0],np.shape(im)[1]])\n",
    "    # read and standardize all images\n",
    "    for i in range(len(files)):\n",
    "        im = np.array(Image.open(files[i]))        \n",
    "        \n",
    "        mean = np.mean(im)\n",
    "        std = np.std(im)\n",
    "        image_stack[i,:,:] = (im - mean)/std\n",
    "        \n",
    "        # for every nth images, explore\n",
    "        if i%100 == 0:\n",
    "            indx = files[i].index('.t')\n",
    "            \n",
    "            # histogram\n",
    "            #hist, bin_edges = np.histogram(im,bins=1000,range=(0,50000))\n",
    "            # to avoid log(0) later\n",
    "            #hist[hist < 1] == 0.1\n",
    "            #bins = (bin_edges[1:]+bin_edges[:-1])/2e0            \n",
    "            \n",
    "            #plt.close()\n",
    "            #width = (bin_edges[1] - bin_edges[0])\n",
    "            #center = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "            #plt.xlabel('pixel intensity')\n",
    "            #plt.ylabel('log(count)')\n",
    "            #plt.xlim([0,50000])\n",
    "            #plt.ylim([0,5])\n",
    "            #plt.bar(bin_edges[:-1], np.log10(hist), width)\n",
    "            #plt.tight_layout(w_pad=0, h_pad=0)\n",
    "            #plt.savefig('imgs/histo_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            #plt.close()\n",
    "            \n",
    "            # LBP\n",
    "            #lbp = local_binary_pattern(im, 8, 1)\n",
    "            #plt.imshow(lbp)\n",
    "            #plt.savefig('imgs/lbp_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            #plt.close()            \n",
    "\n",
    "            # k means clustering\n",
    "            clustered_img = KMeans(2).fit_predict(im.reshape([np.prod(np.shape(im)),1]))            \n",
    "            \n",
    "            clustered_img[clustered_img == clustered_img[0]] = 2e0\n",
    "            clustered_img[clustered_img != clustered_img[0]] = 3e0\n",
    "            \n",
    "            clustered_img = clustered_img.reshape(np.shape(im))\n",
    "            plt.imshow(clustered_img)\n",
    "            plt.savefig('imgs/kmeans_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # label continouos regions\n",
    "            labeled_img, num_regions = ndimage.label(clustered_img)\n",
    "            \n",
    "            plt.imshow(labeled_img)\n",
    "            plt.savefig('imgs/labeled_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # generate the size histogram of different regions\n",
    "            histo = np.zeros(num_regions)\n",
    "            for j in range(num_regions):\n",
    "                histo[j] = np.sum(labeled_img == j)\n",
    "            \n",
    "            histo[histo == 0] = 0.1\n",
    "            histo = np.sort(histo)[::-1]\n",
    "            \n",
    "            plt.xlabel('region label')\n",
    "            plt.ylabel('region size')\n",
    "            plt.xlim([0,20])\n",
    "            plt.ylim([0,6])\n",
    "            plt.bar(range(num_regions), np.log10(histo), np.zeros(num_regions)+1e0)\n",
    "            plt.tight_layout(w_pad=0, h_pad=0)\n",
    "            plt.savefig('imgs/area_histo_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sargentii_17 0\n",
      "sargentii_17 100\n",
      "sargentii_17 200\n",
      "sargentii_17 300\n",
      "sargentii_17 400\n",
      "sargentii_17 500\n",
      "sargentii_17 600\n",
      "sargentii_17 700\n",
      "sargentii_17 800\n",
      "sargentii_17 900\n"
     ]
    }
   ],
   "source": [
    "# pickle dump the clustered image_stack arrays so i don't need to always wait until all images are read and processes.\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "\n",
    "#dirs = ['plicatum_36','sargentii_17']\n",
    "dirs = ['sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = zoom(np.array(Image.open(files[0])),0.5)\n",
    "    \n",
    "    # define image_stack array\n",
    "    # use every second frame and downscale the image\n",
    "    image_stack = np.zeros([len(files)/2+1,np.shape(im)[0],np.shape(im)[1]],dtype='int')\n",
    "    # read images and cluster the pixels\n",
    "    for i in range(len(files)):\n",
    "        if i%100 == 0:\n",
    "            print d,i\n",
    "        \n",
    "        # fill up downscaled array\n",
    "        if i%2 == 0:\n",
    "            # load image\n",
    "            im = zoom(np.array(Image.open(files[i])),0.5)\n",
    "            # do clustering\n",
    "            clustered_img = KMeans(2).fit_predict(im.reshape([np.prod(np.shape(im)),1]))\n",
    "            # make sure the background is 0, the bud is 1, clustering randomizes which one is which\n",
    "            # assuming here that the corner pixel is always background\n",
    "            if clustered_img[0] != 0:\n",
    "                clustered_img ^= 1\n",
    "            # reshape array\n",
    "            clustered_img = clustered_img.reshape(np.shape(im))\n",
    "            # save stack\n",
    "            image_stack[i/2,:,:] = clustered_img\n",
    "\n",
    "    # dump image_stack array\n",
    "    f = open(d+'.dat','w')\n",
    "    pickle.dump(image_stack,f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5c998ff23d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# try marching cubes to extract the surface of the bud\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mayavi import mlab\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "\n",
    "for d in dirs:\n",
    "    f = open(d+'.dat','r')\n",
    "    image_stack = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    zoomed = zoom(image_stack,0.25)\n",
    "\n",
    "    # march the cubes\n",
    "    verts, faces = measure.marching_cubes(zoomed, 0.5)\n",
    "    # GEOM view, visit (check on oscar) - for visualization\n",
    "    \n",
    "    # make the plot with matplotlib\n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    mesh = Poly3DCollection(verts[faces])\n",
    "    ax.add_collection3d(mesh)\n",
    "    ax.set_xlabel(\"x-axis\")\n",
    "    ax.set_ylabel(\"y-axis\")\n",
    "    ax.set_zlabel(\"z-axis\")\n",
    "    ax.set_xlim(0, np.shape(zoomed)[0])  \n",
    "    ax.set_ylim(0, np.shape(zoomed)[1])  \n",
    "    ax.set_zlim(0, np.shape(zoomed)[2]) \n",
    "    plt.savefig('imgs/marching_cubes_'+d+'_plt.png')\n",
    "    plt.close()\n",
    "    \n",
    "    for i in range(360):\n",
    "        # make the plot with mlab\n",
    "        mlab.triangular_mesh([vert[0] for vert in verts],\n",
    "                  [vert[1] for vert in verts],\n",
    "                  [vert[2] for vert in verts], faces) \n",
    "        mlab.view(azimuth = i)\n",
    "        mlab.savefig('ani_'+d+'/ani_'+str(i).zfill(3)+'.png')\n",
    "        mlab.close()\n",
    "        \n",
    "    # to convert the pngs into a movie use\n",
    "    # ffmpeg -framerate 25 -i ani_%03d.png -s:v 640x360 -c:v libx264 -profile:v high -crf 20 -pix_fmt yuv420p ani.mp4\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for oscar: generate verts and faces and save the results in various resolutions\n",
    "# do not run on laptop\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = np.array(Image.open(files[0]))\n",
    "    \n",
    "    # define image_stack array\n",
    "    # use every second frame and downscale the image\n",
    "    image_stack = np.zeros([len(files),np.shape(im)[0],np.shape(im)[1]],dtype='int')\n",
    "    # read images and cluster the pixels\n",
    "    for i in range(len(files)):\n",
    "        if i%100 == 0:\n",
    "            print d,i\n",
    "        \n",
    "        # fill up array\n",
    "        # load image\n",
    "        im = np.array(Image.open(files[i]))\n",
    "        # do clustering\n",
    "        clustered_img = KMeans(2).fit_predict(im.reshape([np.prod(np.shape(im)),1]))\n",
    "        # make sure the background is 0, the bud is 1, clustering randomizes which one is which\n",
    "        # assuming here that the corner pixel is always background\n",
    "        if clustered_img[0] != 0:\n",
    "            clustered_img ^= 1\n",
    "        # reshape array\n",
    "        clustered_img = clustered_img.reshape(np.shape(im))\n",
    "        # save stack\n",
    "        image_stack[i,:,:] = clustered_img\n",
    "\n",
    "    # march the cubes - full resolution\n",
    "    verts, faces = measure.marching_cubes(image_stack, 0.5)\n",
    "    f = open('verts_faces_'+d+'_full_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "\n",
    "    # march the cubes - downscale by a factor of 2\n",
    "    verts, faces = measure.marching_cubes(zoom(image_stack,0.5), 0.5)\n",
    "    f = open('verts_faces_'+d+'_half_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "\n",
    "    # march the cubes - downscale by a factor of 4\n",
    "    verts, faces = measure.marching_cubes(zoom(image_stack,0.25), 0.5)\n",
    "    f = open('verts_faces_'+d+'_quarter_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "\n",
    "    # march the cubes - downscale by a factor of 8\n",
    "    verts, faces = measure.marching_cubes(zoom(image_stack,0.125), 0.5)\n",
    "    f = open('verts_faces_'+d+'_octo_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
