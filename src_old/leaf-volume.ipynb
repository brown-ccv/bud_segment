{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# open and standardize image files, try stuff\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import ndimage\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = np.array(Image.open(files[0]))\n",
    "    \n",
    "    # define image_stack array\n",
    "    image_stack = np.zeros([len(files),np.shape(im)[0],np.shape(im)[1]])\n",
    "    # read and standardize all images\n",
    "    for i in range(len(files)):\n",
    "        im = np.array(Image.open(files[i]))        \n",
    "        \n",
    "        mean = np.mean(im)\n",
    "        std = np.std(im)\n",
    "        image_stack[i,:,:] = (im - mean)/std\n",
    "        \n",
    "        # for every nth images, explore\n",
    "        if i%100 == 0:\n",
    "            indx = files[i].index('.t')\n",
    "            \n",
    "            # histogram\n",
    "            #hist, bin_edges = np.histogram(im,bins=1000,range=(0,50000))\n",
    "            # to avoid log(0) later\n",
    "            #hist[hist < 1] == 0.1\n",
    "            #bins = (bin_edges[1:]+bin_edges[:-1])/2e0            \n",
    "            \n",
    "            #plt.close()\n",
    "            #width = (bin_edges[1] - bin_edges[0])\n",
    "            #center = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "            #plt.xlabel('pixel intensity')\n",
    "            #plt.ylabel('log(count)')\n",
    "            #plt.xlim([0,50000])\n",
    "            #plt.ylim([0,5])\n",
    "            #plt.bar(bin_edges[:-1], np.log10(hist), width)\n",
    "            #plt.tight_layout(w_pad=0, h_pad=0)\n",
    "            #plt.savefig('imgs/histo_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            #plt.close()\n",
    "            \n",
    "            # LBP\n",
    "            #lbp = local_binary_pattern(im, 8, 1)\n",
    "            #plt.imshow(lbp)\n",
    "            #plt.savefig('imgs/lbp_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            #plt.close()            \n",
    "\n",
    "            # k means clustering\n",
    "            clustered_img = KMeans(2).fit_predict(im.reshape([np.prod(np.shape(im)),1]))            \n",
    "            \n",
    "            clustered_img[clustered_img == clustered_img[0]] = 2e0\n",
    "            clustered_img[clustered_img != clustered_img[0]] = 3e0\n",
    "            \n",
    "            clustered_img = clustered_img.reshape(np.shape(im))\n",
    "            plt.imshow(clustered_img)\n",
    "            plt.savefig('imgs/kmeans_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # label continouos regions\n",
    "            labeled_img, num_regions = ndimage.label(clustered_img)\n",
    "            \n",
    "            plt.imshow(labeled_img)\n",
    "            plt.savefig('imgs/labeled_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # generate the size histogram of different regions\n",
    "            histo = np.zeros(num_regions)\n",
    "            for j in range(num_regions):\n",
    "                histo[j] = np.sum(labeled_img == j)\n",
    "            \n",
    "            histo[histo == 0] = 0.1\n",
    "            histo = np.sort(histo)[::-1]\n",
    "            \n",
    "            plt.xlabel('region label')\n",
    "            plt.ylabel('region size')\n",
    "            plt.xlim([0,20])\n",
    "            plt.ylim([0,6])\n",
    "            plt.bar(range(num_regions), np.log10(histo), np.zeros(num_regions)+1e0)\n",
    "            plt.tight_layout(w_pad=0, h_pad=0)\n",
    "            plt.savefig('imgs/area_histo_'+d+'_'+files[i][indx-4:indx]+'.png')\n",
    "            plt.close()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sargentii_17 0\n",
      "sargentii_17 100\n",
      "sargentii_17 200\n",
      "sargentii_17 300\n",
      "sargentii_17 400\n",
      "sargentii_17 500\n",
      "sargentii_17 600\n",
      "sargentii_17 700\n",
      "sargentii_17 800\n",
      "sargentii_17 900\n"
     ]
    }
   ],
   "source": [
    "# pickle dump the clustered image_stack arrays so i don't need to always wait until all images are read and processes.\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "\n",
    "#dirs = ['plicatum_36','sargentii_17']\n",
    "dirs = ['sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = zoom(np.array(Image.open(files[0])),0.5)\n",
    "    \n",
    "    # define image_stack array\n",
    "    # use every second frame and downscale the image\n",
    "    image_stack = np.zeros([len(files)/2+1,np.shape(im)[0],np.shape(im)[1]],dtype='int')\n",
    "    # read images and cluster the pixels\n",
    "    for i in range(len(files)):\n",
    "        if i%100 == 0:\n",
    "            print d,i\n",
    "        \n",
    "        # fill up downscaled array\n",
    "        if i%2 == 0:\n",
    "            # load image\n",
    "            im = zoom(np.array(Image.open(files[i])),0.5)\n",
    "            # do clustering\n",
    "            clustered_img = KMeans(2).fit_predict(im.reshape([np.prod(np.shape(im)),1]))\n",
    "            # make sure the background is 0, the bud is 1, clustering randomizes which one is which\n",
    "            # assuming here that the corner pixel is always background\n",
    "            if clustered_img[0] != 0:\n",
    "                clustered_img ^= 1\n",
    "            # reshape array\n",
    "            clustered_img = clustered_img.reshape(np.shape(im))\n",
    "            # save stack\n",
    "            image_stack[i/2,:,:] = clustered_img\n",
    "\n",
    "    # dump image_stack array\n",
    "    f = open('data_files/'d+'.dat','w')\n",
    "    pickle.dump(image_stack,f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5c998ff23d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# try marching cubes to extract the surface of the bud\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mayavi import mlab\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "\n",
    "for d in dirs:\n",
    "    f = open('data_files/'+d+'.dat','r')\n",
    "    image_stack = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    zoomed = zoom(image_stack,0.25)\n",
    "\n",
    "    # march the cubes\n",
    "    verts, faces = measure.marching_cubes(zoomed, 0.5)\n",
    "    # GEOM view, visit (check on oscar) - for visualization\n",
    "    \n",
    "    # make the plot with matplotlib\n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    mesh = Poly3DCollection(verts[faces])\n",
    "    ax.add_collection3d(mesh)\n",
    "    ax.set_xlabel(\"x-axis\")\n",
    "    ax.set_ylabel(\"y-axis\")\n",
    "    ax.set_zlabel(\"z-axis\")\n",
    "    ax.set_xlim(0, np.shape(zoomed)[0])  \n",
    "    ax.set_ylim(0, np.shape(zoomed)[1])  \n",
    "    ax.set_zlim(0, np.shape(zoomed)[2]) \n",
    "    plt.savefig('imgs/marching_cubes_'+d+'_plt.png')\n",
    "    plt.close()\n",
    "    \n",
    "    for i in range(360):\n",
    "        # make the plot with mlab\n",
    "        mlab.triangular_mesh([vert[0] for vert in verts],\n",
    "                  [vert[1] for vert in verts],\n",
    "                  [vert[2] for vert in verts], faces) \n",
    "        mlab.view(azimuth = i)\n",
    "        mlab.savefig('ani_'+d+'/ani_'+str(i).zfill(3)+'.png')\n",
    "        mlab.close()\n",
    "        \n",
    "    # to convert the pngs into a movie use\n",
    "    # ffmpeg -framerate 25 -i ani_%03d.png -s:v 640x360 -c:v libx264 -profile:v high -crf 20 -pix_fmt yuv420p ani.mp4\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/ndimage/interpolation.py:549: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# for oscar: generate verts and faces and save the results in various resolutions\n",
    "# do not run on laptop\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    f = open('data_files/'+d+'.dat','r')\n",
    "    image_stack = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    ## march the cubes - full resolution\n",
    "    #verts, faces = measure.marching_cubes(image_stack, 0.5)\n",
    "    #f = open('data_files/verts_faces_'+d+'_full_res.dat','w')\n",
    "    #pickle.dump([verts, faces],f)\n",
    "    #f.close()\n",
    "\n",
    "    # march the cubes - downscale by a factor of 2\n",
    "    verts, faces = measure.marching_cubes(zoom(image_stack,0.5), 0.5)\n",
    "    f = open('data_files/verts_faces_'+d+'_half_res.dat','w')\n",
    "    pickle.dump([verts, faces],f)\n",
    "    f.close()\n",
    "\n",
    "    ## march the cubes - downscale by a factor of 4\n",
    "    #verts, faces = measure.marching_cubes(zoom(image_stack,0.25), 0.5)\n",
    "    #f = open('data_files/verts_faces_'+d+'_quarter_res.dat','w')\n",
    "    #pickle.dump([verts, faces],f)\n",
    "    #f.close()\n",
    "\n",
    "    ## march the cubes - downscale by a factor of 8\n",
    "    #verts, faces = measure.marching_cubes(zoom(image_stack,0.125), 0.5)\n",
    "    #f = open('data_files/verts_faces_'+d+'_octo_res.dat','w')\n",
    "    #pickle.dump([verts, faces],f)\n",
    "    #f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# work with the verts, faces\n",
    "# separate the unconnected surfaces, and clean the data by removing the small isolated surfaces (noise).\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "from mayavi import mlab\n",
    "\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "res = 'half'\n",
    "\n",
    "def surface_finder(faces,surface,elements_removed):\n",
    "    unique_elements = np.unique(surface)\n",
    "    elements_to_check = np.setdiff1d(unique_elements,elements_removed,assume_unique=True)\n",
    "    if np.all([np.any(e not in faces) for e in elements_to_check]):\n",
    "        return faces, surface, elements_removed\n",
    "    else:\n",
    "        indcs = np.any([np.any(faces == e,axis=1) for e in elements_to_check],axis=0)\n",
    "        surf = faces[indcs,:]\n",
    "        new_surface = np.concatenate((surface,surf),axis=0)\n",
    "        new_faces = faces[~indcs,:]\n",
    "        elements_removed = np.concatenate((elements_removed,elements_to_check))\n",
    "        return surface_finder(new_faces,new_surface,elements_removed)\n",
    "\n",
    "for d in dirs:\n",
    "\n",
    "    f = open('data_files/verts_faces_'+d+'_'+res+'_res.dat','r')\n",
    "    [verts, faces] = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # verts are a list of 3D point coordinates.\n",
    "    # faces describe which 3 points form a triangle that should be shaded.\n",
    "    # e.g., this is one triangle:\n",
    "    # verts = [[  1.    3.   32.5]\n",
    "    # [  0.5   3.   33. ]\n",
    "    # [  1.    2.5  33. ]]\n",
    "    # faces = [[ 0  1  2]]\n",
    "    # point 0 is [  1.    3.   32.5]\n",
    "    # point 1 is [  0.5   3.   33. ]\n",
    "    # point 2 is [  1.    2.5  33. ]\n",
    "    # faces [ 0  1  2] describes that these three points form a triangle.\n",
    "    #\n",
    "    # separate surfaces do not have common points.\n",
    "    \n",
    "    print np.shape(verts)\n",
    "    print np.shape(faces)\n",
    "    \n",
    "    # find separate surfaces\n",
    "    surfaces = []\n",
    "    \n",
    "    while np.shape(faces)[0] > 0:\n",
    "        print 'new surface',np.shape(faces)[0]\n",
    "        surface = [faces[0,:]]\n",
    "        faces = np.delete(faces,0,axis=0)    \n",
    "        elements_removed = np.empty(0,dtype=int)\n",
    "\n",
    "        faces, surface, elements_removed = surface_finder(faces,surface,elements_removed)\n",
    "        #unique_elements = np.unique(surface)\n",
    "        #if np.any([np.any(e in faces) for e in unique_elements]):\n",
    "        #    print 'there are points left in faces that should be in surfaces'\n",
    "        #    print unique_elements\n",
    "        #    print np.shape(surface),np.shape(faces)\n",
    "        #    raise ValueError\n",
    "        surfaces.append(surface)\n",
    "    \n",
    "    f = open('data_files/surfaces_'+d+'_'+res+'_res.dat','w')\n",
    "    pickle.dump(surfaces,f)\n",
    "    f.close()\n",
    "    \n",
    "    surf_size = np.zeros(len(surfaces))\n",
    "    for i in range(len(surfaces)):\n",
    "        surf_size[i] = np.shape(surfaces[i])[0]\n",
    "    \n",
    "    print surf_size\n",
    "    stop\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24684\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:21: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:22: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:22: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-68279313a78b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# remove faces that connect to the outermost points.\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "from mayavi import mlab\n",
    "import itertools\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "res = 'octo'\n",
    "\n",
    "\n",
    "def point_in_triangle(p,p0,p1,p2):\n",
    "    area = np.abs(1e0/2e0*(-p1[1]*p2[0] + p0[1]*(-p1[0] + p2[0]) + p0[0]*(p1[1] - p2[1]) + p1[0]*p2[1]))\n",
    "    s = 1e0/(2e0*area)*(p0[1]*p2[0] - p0[0]*p2[1] + (p2[1] - p0[1])*p[0] + (p0[0] - p2[0])*p[1])\n",
    "    t = 1e0/(2e0*area)*(p0[0]*p1[1] - p0[1]*p1[0] + (p0[1] - p1[1])*p[0] + (p1[0] - p0[0])*p[1])\n",
    "    #print area,s,t,1e0-s-t\n",
    "    if s>=0 and t>=0 and 1e0-s-t>=0:\n",
    "        #print p,p0,p1,p2\n",
    "        return True\n",
    "    else:\n",
    "        #print p,p0,p1,p2\n",
    "        return False\n",
    "    \n",
    "\n",
    "for d in dirs:\n",
    "\n",
    "    f = open('data_files/verts_faces_'+d+'_'+res+'_res.dat','r')\n",
    "    [verts, faces] = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    f = open('data_files/surfaces_'+d+'_'+res+'_res.dat','r')\n",
    "    surfaces = pickle.load(f)\n",
    "    f.close()\n",
    "        \n",
    "    surf_size = np.zeros(len(surfaces))\n",
    "    for i in range(len(surfaces)):\n",
    "        surf_size[i] = np.shape(surfaces[i])[0]\n",
    "    \n",
    "    # choose the surface with the largest size\n",
    "    surface = surfaces[np.argmax(surf_size)]\n",
    "    \n",
    "    # remove the outer faces \n",
    "    # the first dimension is the z coordinate, the second and third dimensions are the x and y coordinates\n",
    "    # calculate the average y coordinates of each surface \n",
    "    # go through each x-z grid center and mark the surface that has the min and max average y coordinate for deletion\n",
    "    \n",
    "    y_avg = np.average(verts[surface,-1],axis=1)\n",
    "    mark = np.zeros(np.shape(surface)[0],dtype=bool)\n",
    "    z_unique = np.unique(verts[surface,0])\n",
    "    x_unique = np.unique(verts[surface,1])\n",
    "    indx = np.arange(np.shape(surface)[0])\n",
    "    grids = list(itertools.product(z_unique,x_unique))\n",
    "\n",
    "    print len(grids)\n",
    "    ii = 0\n",
    "    for g in grids:\n",
    "        if ii%100 == 0:\n",
    "            print ii\n",
    "        in_triangle = np.zeros(np.shape(surface)[0],dtype=bool)\n",
    "        for i in range(np.shape(surface)[0]):            \n",
    "            p0 = [verts[surface[i,0],0],verts[surface[i,0],1]]\n",
    "            p1 = [verts[surface[i,1],0],verts[surface[i,1],1]]\n",
    "            p2 = [verts[surface[i,2],0],verts[surface[i,2],1]]\n",
    "            in_triangle[i] = point_in_triangle(g,p0,p1,p2)\n",
    "        # collect y_avg values that belong to points within triangle\n",
    "        if np.sum(in_triangle) > 0:\n",
    "            indcs = indx[in_triangle]   \n",
    "            \n",
    "            indx_max = indcs[y_avg[indcs] == np.max(y_avg[indcs])]\n",
    "            mark[indx_max] = 1\n",
    "            \n",
    "            indx_min = indcs[y_avg[indcs] == np.min(y_avg[indcs])]\n",
    "            mark[indx_min] = 1\n",
    "                        \n",
    "            #print y_avg[indx_max]\n",
    "            #print y_avg[indx_min]\n",
    "            #print 'sums:',np.sum(in_triangle),len(indx)\n",
    "        ii = ii + 1\n",
    " \n",
    "    mark = np.array(mark,dtype=bool)\n",
    "    new_surf = surface[~mark,:]\n",
    "\n",
    "    f = open('data_files/surface_outer_removed_'+d+'_'+res+'_res.dat','w')\n",
    "    pickle.dump(new_surf,f)\n",
    "    f.close()\n",
    "\n",
    "    stop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 2]\n",
      "[[ 23.5  23.5  24. ]\n",
      " [ 23.5  24.   24. ]\n",
      " [ 24.   25.   24. ]\n",
      " ..., \n",
      " [ 29.5  30.   30. ]\n",
      " [ 30.   31.   30. ]\n",
      " [ 30.   31.   30.5]]\n",
      "[ 30.   31.   30.5]\n",
      "3.5 44.0\n",
      "(118374,)\n"
     ]
    }
   ],
   "source": [
    "print surface[1,:]\n",
    "#print verts[surface,:][:,:,:]\n",
    "print verts[surface,-1]\n",
    "print verts[surface[-1,:],-1]\n",
    "print z,x\n",
    "print np.shape(mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data_files/surfaces_outer_removed_sargentii_17_octo_res.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c1d6ca5aee5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_files/surfaces_outer_removed_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_res.dat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mnew_surf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data_files/surfaces_outer_removed_sargentii_17_octo_res.dat'"
     ]
    }
   ],
   "source": [
    "# make the movie of various surfaces\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "from mayavi import mlab\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "res = 'octo'\n",
    "\n",
    "for d in dirs:\n",
    "\n",
    "    f = open('data_files/verts_faces_'+d+'_'+res+'_res.dat','r')\n",
    "    [verts, faces] = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    f = open('data_files/surfaces_outer_removed_'+d+'_'+res+'_res.dat','r')\n",
    "    new_surf = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    #surf_size = np.zeros(len(surfaces))\n",
    "    #for i in range(len(surfaces)):\n",
    "    #    surf_size[i] = np.shape(surfaces[i])[0]\n",
    "    \n",
    "    #sorted_indcs = np.argsort(surf_size)\n",
    "        \n",
    "    # make the plot with mlab\n",
    "    for i in range(9):\n",
    "        mlab.triangular_mesh([vert[0] for vert in verts],\n",
    "              [vert[1] for vert in verts],\n",
    "              [vert[2] for vert in verts], new_surf) \n",
    "        mlab.view(azimuth = i*40)\n",
    "        mlab.savefig('ani_'+d+'/ani_outer_removed_'+str(i).zfill(3)+'.png')\n",
    "        mlab.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "[3L, 992L, 1013L, 1906L, 1L, 1L, 1L, 1L]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nifti/image.py:231: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  return (not self._data == None)\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/ndimage/interpolation.py:549: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3L, 496L, 507L, 953L, 1L, 1L, 1L, 1L]\n",
      "[3L, 248L, 253L, 477L, 1L, 1L, 1L, 1L]\n",
      "[3L, 124L, 127L, 238L, 1L, 1L, 1L, 1L]\n",
      "done\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c6a14502cf4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'done'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# save the image stack as a nifti file so i can try ITK-SNAP to segment the image\n",
    "\n",
    "from nifti import *\n",
    "import nifti.clib as ncl\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "\n",
    "dirs = ['plicatum_36','sargentii_17']\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = np.array(Image.open(files[0]))\n",
    "    \n",
    "    # define image_stack array\n",
    "    image_stack = np.zeros([len(files),np.shape(im)[0],np.shape(im)[1]])\n",
    "    # read and standardize all images\n",
    "    for i in range(len(files)):\n",
    "        if i%100 == 0:\n",
    "            print i\n",
    "        im = np.array(Image.open(files[i]))\n",
    "        image_stack[i,:,:] = im\n",
    "            \n",
    "    nim = NiftiImage(image_stack)\n",
    "    print nim.header['dim']\n",
    "    nim.header['datatype'] == ncl.NIFTI_TYPE_FLOAT64\n",
    "    nim.save('data_files/'+d+'_full_res.nii.gz')\n",
    "\n",
    "    nim = NiftiImage(zoom(image_stack,0.5))\n",
    "    print nim.header['dim']\n",
    "    nim.header['datatype'] == ncl.NIFTI_TYPE_FLOAT64\n",
    "    nim.save('data_files/'+d+'_half_res.nii.gz')\n",
    "    \n",
    "    nim = NiftiImage(zoom(image_stack,0.25))\n",
    "    print nim.header['dim']\n",
    "    nim.header['datatype'] == ncl.NIFTI_TYPE_FLOAT64\n",
    "    nim.save('data_files/'+d+'_quarter_res.nii.gz')\n",
    "\n",
    "    nim = NiftiImage(zoom(image_stack,0.125))\n",
    "    print nim.header['dim']\n",
    "    nim.header['datatype'] == ncl.NIFTI_TYPE_FLOAT64\n",
    "    nim.save('data_files/'+d+'_octo_res.nii.gz')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print 'done'\n",
    "    stop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(477, 253, 248)\n"
     ]
    }
   ],
   "source": [
    "# animate the ITK-SNAP output\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import measure\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mayavi import mlab\n",
    "from nifti import NiftiImage\n",
    "\n",
    "dirs = ['plicatum_36']\n",
    "\n",
    "\n",
    "for d in dirs:\n",
    "    nim = NiftiImage('data_files/plicatum_36_quarter_segmented.nii').asarray()\n",
    "    \n",
    "    print np.shape(nim)\n",
    "    \n",
    "\n",
    "    # make the plot with matplotlib\n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_xlabel(\"x-axis\")\n",
    "    ax.set_ylabel(\"y-axis\")\n",
    "    ax.set_zlabel(\"z-axis\")\n",
    "    ax.set_xlim(0, np.shape(nim)[0])  \n",
    "    ax.set_ylim(0, np.shape(nim)[1])  \n",
    "    ax.set_zlim(0, np.shape(nim)[2]) \n",
    "    \n",
    "    for i in range(360):\n",
    "        # make the plot with mlab\n",
    "        mlab.contour3d(nim)\n",
    "        mlab.view(azimuth = i)\n",
    "        mlab.savefig('ani_'+d+'/ani_leaf_'+str(i).zfill(3)+'.png')\n",
    "        mlab.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_stacks/plicatum_36']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-554db0ded67c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# save the image stacks I got from Erika as a nifti file so i can try ITK-SNAP to segment the image\n",
    "\n",
    "from nifti import *\n",
    "import nifti.clib as ncl\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "dirs = [x[0] for x in os.walk('image_stacks/')][1:]\n",
    "\n",
    "stop\n",
    "\n",
    "for d in dirs:\n",
    "    files = []\n",
    "    for file in glob.glob(d+'/*.ti*'):\n",
    "        files.append(file)\n",
    "    \n",
    "    # open first image to get the image dimensions\n",
    "    im = np.array(Image.open(files[0]))\n",
    "    \n",
    "    # define image_stack array\n",
    "    image_stack = np.zeros([len(files),np.shape(im)[0],np.shape(im)[1]])\n",
    "    # read and standardize all images\n",
    "    for i in range(len(files)):\n",
    "        if i%100 == 0:\n",
    "            print i\n",
    "        im = np.array(Image.open(files[i]))\n",
    "        image_stack[i,:,:] = im\n",
    "            \n",
    "    #nim = NiftiImage(image_stack)\n",
    "    #print nim.header['dim']\n",
    "    #nim.header['datatype'] == ncl.NIFTI_TYPE_FLOAT64\n",
    "    #nim.save('data_files/'+d+'_full_res.nii.gz')\n",
    "\n",
    "    #nim = NiftiImage(zoom(image_stack,0.5))\n",
    "    #print nim.header['dim']\n",
    "    #nim.header['datatype'] == ncl.NIFTI_TYPE_FLOAT64\n",
    "    #nim.save('data_files/'+d+'_half_res.nii.gz')\n",
    "    \n",
    "    nim = NiftiImage(zoom(image_stack,0.25))\n",
    "    print nim.header['dim']\n",
    "    nim.header['datatype'] == ncl.NIFTI_TYPE_FLOAT64\n",
    "    nim.save('data_files/'+d+'_quarter_res.nii.gz')\n",
    "\n",
    "    #nim = NiftiImage(zoom(image_stack,0.125))\n",
    "    #print nim.header['dim']\n",
    "    #nim.header['datatype'] == ncl.NIFTI_TYPE_FLOAT64\n",
    "    #nim.save('data_files/'+d+'_octo_res.nii.gz')\n",
    "    \n",
    "    print 'done with ',d\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
